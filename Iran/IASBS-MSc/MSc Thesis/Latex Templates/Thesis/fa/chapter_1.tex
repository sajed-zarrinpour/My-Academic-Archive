\section*{مقدمه}
\addcontentsline{toc}{section}{مقدمه}
مقدار سنجي عدم قطعيت (۱۵) در فيزيک و مهندسي اغلب شامل مطالعه معادلات ديفرانسيل با مشتقات جزئي با ميدان ضرايب تصادفي است. براي درک رفتار يک سيستم شامل عدم قطعيت، مي‌توان کميت‌هاي فيزيکي مشتق شده از معادلات ديفرانسيل توصیف کننده آن سیستم را به عنوان توابعي از ميدان ضرايب استخراج کرد. اما حتي با گسسته‌سازي مناسب روي دامنه معادله و برد متغييرهاي تصادفي، اين کار به طور ضمني به حل عددي معادله ديفرانسيل با مشتق جزئي به تعداد نمايي مي‌انجامد.
يکي از روش‌هاي متداول براي مقدار سنجي عدم قطعيت روش نمونه‌ برداري مونته کارلو است. گرچه اين روش در بسياري از موارد کاربردي است اما کميت اندازه‌گيري شده ذاتاً داراي پراش است. به‌علاوه اين روش قادر به پيدا کردن جواب‌هاي جديد در صورتي که قبلا نمونه‌گيري نشده باشند، نيست.
روش گالرکين تصادفي با استفاده چند جمله‌اي‌هاي آشوب  يک جواب تصادفي را روي فضاي متغييرهاي تصادفي بسط مي‌دهد و به اين طريق مسئله با بعد بالا را به تعدادي معادله ديفرانسيل با مشتقات جزئي معين تبديل مي‌کند. اين گونه روش‌ها به دقت زيادي درباره تعيين توزيع عدم قطعيت نيازمند هستند و از آن‌جا که پايه‌هاي استفاده شده مستقل از مسئله هستند، وقتي بعد متغييرهاي تصادفي بالا باشد هزينه محاسباتي بسيار زياد خواهد شد.
هدف کار ما پارامتري کردن جواب يک معادله ديفرانسيل معيين به کمک شبکه‌هاي عصبي و سپس استفاده از روش‌هاي بهينه‌سازي براي يافتن جواب معادله است. در این پایان‌نامه تابع مورد نظر براي پارامتري‌سازي روي ميدان ضرايب معادله ديفرانسيل با مشتقات جزئي تعريف شده است. در واقع ما به دنبال کاهش بعد مبتني بر نمايش شبکه ‌عصبي براي حل معادلات ديفرانسيل با مشتقات جزئي همراه با عدم قطيت هستیم.
\clearpage
\newpage
\section*{انگیزه و هدف}
\addcontentsline{toc}{section}{انگیزه و هدف}
مدل‌سازی طبیعت همیشه با پارامترهایی همراه است که مقادیر آنها از کنترل ما خارج است. اما عموما ما درباره محدوده تغییرات این پارامترها اطلاعاتی داریم. به معادلاتی که شامل این‌گونه پارامترهایی هستند، معادلات با ضرایب عدم قطعیت گوییم. به طور مثال، در مورد حرکت مایعات، به طور مثال نفت، در سفره‌های زیر زمینی، برای بیان شیوه حرکت مایعات نیاز به دانستن مکان حفره‌ها داریم. این امر را می‌توان به صورت رسانایی مؤثر در حضور ناخالصی نیز در نظر گرفت. به عنوان مثالی دیگر، مسئله‌ای را مطرح می‌کنیم که نقطه شروع این رساله بوده است. 
برای تشخیص سرطان پستان روش‌های متعددی موجود است. از جمله آن‌ها می‌توان به تصویر برداری پستان با بازتابش اشعه ایکس (XRM)\LTRfootnote{Projection X-ray mammography}، تصویربرداری با استفاده از ارتعاشات مغناطیسی (MRI)\LTRfootnote{Magnetic resonance imaging}، تصویربرداری فراصوت (US)\LTRfootnote{Ultra sound}، توموسنتز دیجیتال (DBT)\LTRfootnote{Digital breast tomosynthesis}، ماموگرافی انتشار پوزیترون (PET)\LTRfootnote{Positron emission mammography} و توموگرافی فراصوت (UST)\LTRfootnote{Ultra sound tomography} اشاره کرد. هرکدام از این روش‌ها اطلاعات را به طرق مختلفی نمایش می‌دهند، به این معنا که غده‌ای که در یکی از این روش‌ها غیر قابل تشخیص است در روش دیگر قابل تشخیص است؛ غده‌ای که در یک روش بافت مشکوک معرفی می‌شود، در روش دیگر می‌تواند به عنوان غده‌ای سالم و طبیعی معرفی شود. و این موضوع باعث ایجاد مشکلات بسیاری در روند تشخیص و برنامه ریزی درمان می‌شود. در این مرحله، راه‌حلی که به ذهن می‌رسد، ترکیب نتایج حاصل از این روش‌ها برای بالابردن ضریب دقت است؛ لیکن مشکل دیگری مانع این‌کار می‌شود. بافت پستان بسیار کشسان است به راحتی تغییر فرم می‌دهد و هرکدام از این روش‌ها نیز به حالت خاصی از قرارگیری بیمار نیاز دارد. به طور مثال، طی MRI بیمار در حالت دمر قرار دارد ولی برای تصویربرداری فراصوت بیمار به پشت می‌خوابد. علاوه‌براین، در روش بایوپسی راهنمایی شده توسط MRI\LTRfootnote{MRI-guided biopsy} بافت پستان توسط صفحه‌های سخت و غیرقابل انعطافی بی حرکت می‌شوند که منجر به فشرده شدن بافت نیز می‌شود. بنابراین، شکل، اندازه و مکان غده در این تصاویر متفاوت خواهد بود. این امر مقایسه تصاویر را با سختی بسیار همراه می‌کند. علاوه‌براین، برای برنامه‌ریزی پیش از جراحی، پزشک نیاز به دانستن مکان و اندازه دقیق غده دارد. بنابراین، نیاز به توسعه الگوریتم‌های ثبت غیرسخت\LTRfootnote{Non-rigid registration algorithm} احساس می‌شود. 
روش‌هائی مبتنی بر روش المان‌های متناهی\LTRfootnote{Finite element method} برای حل این مسئله ارائه شده‌اند. اما مشکل عمده این روش‌ها هزینه محاسباتی بالای آنهاست. به طور متوسط اجرای  یک شبیه سازی صد و بیست دقیقه به طول می‌انجامد که مقرون به صرفه نیست. ما به دنبال ارائه روشی برای کاهش این هزینه محاسباتی با استفاده از شبکه‌های عصبی و ارائه یک مدل مختص به بیمار در زمانی قابل قبول بودیم. این امر مستلزم در نظر گرفتن ضریب کشسانی بدن بیمار، که یک ضریب عدم قطعیت است، می‌باشد. از این رو، برآن شدیم که بدنبال حل عددی معادلات دیفرانسیل (بیضوی) به کمک شبکه‌های عصبی باشیم. 
در این رساله، بدنبال حل عددی معادلات دیفرانسیل با مشتقات جزئی بیضوی خطی و غیر خطی ناهمگن هستیم. روشی که ما در سدد معرفی آن هستیم، یک روش کاهش بعد برای محاسبه جواب بدون نیاز به حل مستقیم معادله دیفرانسیل است. ایده، استفاده از شبکه‌های عصبی برای یادگیری نگاشتی از دامنه ضرایب عدم قطعیت به فضای جواب بر اساس مجموعه داده‌ای از قبل محاسبه شده است.
%\section*{روش‌های تقریبی }
%\addcontentsline{toc}{section}{روش‌های تقریبی}
\clearpage
\newpage
\section*{تعریف مسئله }
\addcontentsline{toc}{section}{تعریف مسئله}
در این پایان نامه، هدف ما بررسی یک مدل میانبر برای حل مسائل معادلات دیفرانسیل با مشتقات جزئی به کمک شبکه‌های عصبی بوده است. به عبارت روشن‌تر یافت نگاشتی از فضای عدم قطعیت مسئله به فضای جواب. مسائلی که در این رساله برای حل انتخاب شده اند از این جهت حائز اهمیت بوده‌اند که هر دو حالت خطی و غیر خطی معادلات دیفرانسیل غیر همگن شامل عدم قطعیت را پوشش می‌دهند. \\
\subsection*{یافت ضریب رسانایی مؤثر در محیط ناهمگون}
معادله اول، رسانایی مؤثر در یک جهت انتخاب شده درون یک ماده غیر همگون را توسط ضریب رسانش توصیف می‌کند. ماده غیر همگون، ماده‌ای است که در آن خصوصیات مورد توجه در تمامی نقاط یکسان نیستد. این امر ممکن است به دلایلی همچون جنس‌های گوناگون مواد تشکیل دهنده یا چگالی های متفاوت مربوط باشد. فرض ما بر آن است که ضریب رسانایی ماده در جهات متفاوت یکسان نباشد و این ضریب را با $a(x)$ نمایش می‌دهیم. با فرض انتخاب یک جهت دلخواه ثابت $\xi \in \mathbb{R}^d$، میزان ضریب رسانش در آن جهت مطلوب است. به عبارت دقیق‌تر، جواب معادله زیر مد نظر است:
\begin{equation*}
A_{\text{eff}}(a) = \min_{u(x)} \int_{[0,1]^d} a(x) ||\nabla u(x) + \xi||_{2}^{2} \mathrm{d}x.
\end{equation*}
\subsection*{یافت ضریب رسانایی مؤثر در محیط ناهمگون}
معادله دوم، معادله غیرخطی شرودینگر دو بعدی است. هدف از این معادله، یافت میزان انرژی حالت پایه الکترون با پتانسیل اولیه همراه با عدم قطعیت است. این معادله به صورت یک مسئله مقدار ویژه به صورت زیر تعریف می‌شود، که هدف ما در حل این مسئله یافتن کوچکترین مقدار ویژه آن است:
\begin{equation}
-\Delta u(x) + a(x)u(x) + \sigma u(x)^3 = E_0 u(x), x\in [0,1]^d, s.t. \int_{[0,1]^d}u(x)^2 dx = 1.
\end{equation}
 \clearpage
 \newpage
\section*{شبکه‌های عصبی }
\addcontentsline{toc}{section}{شبکه‌های عصبی }
شبکه عصبی از تعدادی واحد متصل به هم نام نورون تشکیل می‌شود. هر نورون دارای یک وضعیت داخلی است که در ترکیب با داده ورودی تغییر می‌کند و خروجی نورون را به حالت روشن یا خاموش تغیر می‌دهد. به عبارت ریاضی، هر نورون دارای ضرایب داخلی به نام وزن و بایاس است که به ترتیب با $W$ و $b$ نمایش داده می‌شوند. و خروجی نورون در این صورت با فرض اینکه $X$ ورودی نورون باشد عبارت خواهد بود از $\phi(WX + b)$. در این صورت، می‌توان روند یادگیری یک شبکه عصبی را معادل با یک مسئله کمینه‌سازی در نظر گرفت. به شبکه رابطه‌ای روی داده های خروجی میدهیم تا آن را کمینه کند و ابزار شبکه برای کمینه سازی آن رابطه، تغییر وزن‌ها و بایاس‌های نورون‌های خود است. قضیه زیر که به قضیه تقریب جهانی مشهور است، این را بیان می‌کند که می‌توان تحت شرایطی از شبکه‌های عصبی برای تقریب جواب مسئله استفاده کرد:\\
\textbf{قضیه}. (قضیه تقریب جهانی) \\
	\begin{enumerate}
	\item \textbf{(حالت نامتناهی)} فرض کنید $\varphi :\mathbb {R} \to \mathbb {R}$ یک تابع غیرثابت پیوسته بی کران باشد که آن را تابع فعال سازی می‌نامیم. فرض کنید $I_{m}$ بیان‌گر ابر مکعب $m$-بعدی $[0,1]^{m}$ باشد، و فضای توابع پیوسته حقیقی مقدار روی $I_m$ با $C(I_{m})$ نمایش داده شود. در این‌صورت، به ازای هر $\varepsilon > 0$ دلخواه و هرتابع  $f\in C(I_{m})$, ثوابت حقیقی مانند  $v_{i},b_{i}\in \mathbb  {R}$ و بردارهای $w_{i}\in \mathbb {R} ^{m}$ برای $i=1,\dots ,N$وجود دارند، به طوری‌که می‌توانیم  	
	\begin{equation*}
	F(x)=\sum _{{i=1}}^{{N}}v_{i}\varphi \left(w_{i}^{T}x+b_{i}\right)
	\end{equation*}
	را به عنوان تقریبی از $f$ ارائه دهیم که عبارت است از:
	\begin{equation*}
	|F(x) - f (x)|  < \varepsilon
	\end{equation*}
	که در آن $x\in I_{m}$ است. به عبارت دیگر، توابع به شکل $F(x)$ در $C(I_{m})$ چگال‌اند.\\
	این نتیجه به ازای هر زیر محموعه فشرده دیگری از $\mathbb {R} ^{m}$ به جای  $I_m$ برقرار است.
	\item \textbf{(حالت کران‌دار)} در شبکه‌های کران‌دار، برای هرتابع انتگرال‌پذیر لبگ مانند  $f:\mathbb {R} ^{n}\rightarrow \mathbb {R}$ و هر $\epsilon >0$یک شبکه ReLu کامل $\mathcal {A}$ با عرض $ d_{m}\leq {n+4}$, به گونه ای موجود است که $F_{\mathcal {A}}$ نمایش داده شده با این شبکه در رابطه
	\begin{equation*}
	\int _{\mathbb {R} ^{n}}\left|f(x)-F_{\mathcal {A}}(x)\right|\mathrm {d} x<\epsilon
	\end{equation*}
	صدق ‌نماید.
\end{enumerate}
\clearpage
\newpage
\section*{روش پیشنهادی }
\addcontentsline{toc}{section}{روش پیشنهادی }
معادله تعیین ضریب رسانش مؤثر تنها در یک بعد، و معادله شرودینگر در دو بعد حل خواهند شد. ابتدا معادلات فوق را با روش‌ تفاضلات متناهی (و یا هر روش عددی دیگری) حل نموده و یک پایگاه داده می‌سازیم. برای اینکار، معادله تعیین ضرایب رسانش مؤثر روی یک شبکه نه نقطه‌ای متساوی‌الفاصله، با مقادیر ضرایب عدم قطعیت با توزیع نرمال $\mathscr{U}[0.3, 1.5]$، و معادله شرودینگر غیر خطی روی یک شبکه هشتاد و یک نقطه‌ای متساوی‌الفاصله (گسسته سازی نه نقطه‌ای هر کدام از ابعاد)با مقادیر ضرایب عدم قطعیت با توزیع نرمال $\mathscr{U}[1, 16]$ به تعداد نمونه‌های مورد نیاز حل می‌شوند. سپس درصدی از تکرارها (در اینجا هفتاد و پنج درصد) به عنوان داده برای مرحله آموزش و الباقی برای مرحله آزمون کنار گذاشته می‌شوند.\\
شبکه عصبی متشکل از سه بخش است. بخش اول و سوم قرینه یکدیگر و متشکل از لایه‌های پیچشی \LTRfootnote{convolutional layers}  اند که به واسطه بخش دوم که یک استخر مجموع \LTRfootnote{sum-pooling} است به هم متصل شده‌اند. ورودی این شبکه برای رسانایی مؤثر و یک ماتریس برای معادله شرودینگر است. دقت شود که در حالت دو بعدی، قبل از لایه‌های پیچشی، ابعاد داده ورودی گسترش می‌یابد. این امر با توجه به اینکه شرط مرزی مسئله دوره‌ای است، به صورت کاشی کاری دوره‌ای انجام می‌شود. خروجی شبکه در هر دو حالت یک اسکالر است. که در مورد ضزیب رسانائی مؤثر، این اسکالر برابر ضریب رسانائی مؤثر در جهت ثابت $\xi$ و در مورد معادله شرودینگر، برابر با سطح انرژی پایه است.\\
شبکه پس از چندین بار مرور داده ها در انتها ضرایب خود را به گونه ای تنظیم میکند که تابع هدفی که به آن معرفی کرده ایم را کمینه نماید. وقتی تابع مذکور به میزان کمینه خود برسد می‌گوییم آموزش شبکه به اتمام رسیده است. از این پس می‌توانیم با خوراندن ورودی جدید به شبکه از آن برای یافت جواب استفاده نماییم.\\
\clearpage
\newpage
\section*{نتایج}
\addcontentsline{toc}{section}{نتایج}
مقادیر ضریب رسانایی مؤثر و انرژی حالت پایه به ترتیب $0.76800650$ و $10.17474556$ بدست آمده اند که خطای $L^2$ به ترتیب عبارت اند از $1.02100 \times 10^{-3}$ و $7.235 \times 10^{-5}$. نمودار توزیع خطا بر حسب نمونه برای ضریب رسانایی مؤثر به شرح زیر است:
\begin{figure}[h!]
	{
		\centering
		\def\svgwidth{\columnwidth}
		\scalebox{.5}{\input{images/ECIM_bar.pdf_tex}}
		\caption{خطای مرتکب شده  روی مجموعه آزمون به تفکیک نمونه}
		\label{fig:ECIM_error_bar}
	}
\end{figure}
\\همچنین، نمودار مشابه برای انرژی حالت پایه نیز به شرح است:
\begin{figure}[h!]
	{
		\centering
		\def\svgwidth{\columnwidth}
		\scalebox{.5}{\input{images/bar.pdf_tex}}
		\caption{خطای مرتکب شده  روی مجموعه آزمون به تفکیک نمونه}
		\label{fig:NLSE_error_bar}
	}
\end{figure}
\clearpage
\newpage
\section*{نتیجه‌گیری و کارهای پیش‌رو}
\addcontentsline{toc}{section}{نتیجه‌گیری و کارهای پیش‌رو}
همان‌گونه که از نتایج مشهود است، شبکه‌های عصبی توانایی بالایی در تقریب روابط پنان مابین داده‌ها دارند. همچنین سادگی روش، آن را به یک روش در دسترس تبدیل می‌کند. ضمن اینکه پس از طی مرحله آموزش، شبکه عصبی قادر است جواب مسئله را تقریبا به طور آنی ارائه کند. یکی از محدودیت‌های شبکه‌های عصبی در مورد اندازه مقیاس ورودی‌هاست: به این معنی که در صورتی که برچسب‌ها بسیار بزرگ باشند یا با فاصله بسیار از هم روند یادگیری با مشکل مواجه می‌شود. همان‌گونه که مشاهده می‌شود خطا در معادله شرودینگر به علت بزرگ بودن برچسب ها در مقایسه با ضرایب عدم قطعیت بیشتر است. ما از یک روش نرمال‌سازی برای نرمال‌سازی استفاده نمودیم. چه روش‌های دیگری برای حل این مسئله موجود است و آیا این نرمال سازی خود خطایی به مدل تحمیل می‌کند؟ کران این خطای تحمیلی چیست؟
\clearpage
\newpage