\chapter{Mathematical Preliminaries}
In this chapter, the required mathematical background to understand the rest of the thesis has discussed. It begins with functional analysis. It will be use to show the convergence of the NN model. Then, It goes over the mathematics of the FEM to demonstrate the necessity of developing new methods that can perform better than our classical methods. This chapter has been concluded by the universal approximation theorem that ensures the existence of a neural network that approximates the solution. 
\section{Functional Analysis}
\label{sec:functional_analysis}
This section begins with introducing measures and follows by  Lebesgue integration and functional spaces. This section has been heavily influenced by \cite{UQIntro_Sullivan}. Interested readers may go further by reading \cite{UQIntro_Sullivan, rudin1991functional}.
\subsection{Measure Spaces}
Sample spaces are abstract sets. One can label certain subsets of these sets with a numerical notion of their `size' and call them `measurable'. To be exact  
\begin{defn}
	A \textit{measurable space} is a pair $(\mathscr{X}, \mathscr{F})$, where
	\begin{enumerate}[i.]
		\item $\mathscr{X}$ is a set (the sample space),
		\item $\mathscr{F}$ is a $\sigma-algebra$ on $\mathscr{X}$, (i.e. collection of subsets of $\mathscr{X}$ containing $\phi$ and closed under applying
countable operations of union, intersection, and complementation relative to $\mathscr{X}$).
	\end{enumerate}
\end{defn}
%\begin{defn}
%	\begin{enumerate}[i.]
%		\item A \textit{signed measure} (or \textit{charge}) on a measurable space $(\mathscr{X}, \mathscr{F})$ is a
%function $\mu : \mathscr{F} \to \mathbb{R} \bigcup \{\pm\infty\}$ that takes at most one of the two infinite values, has $\mu(\phi) = 0$, and,
%whenever $E_1, E_2, \ldots \in \mathscr{F}$ are pairwise disjoint with union $E \in \mathscr{F}$,
%then
%$\mu(E) =
%\sum_{n \in N} \mu(E_n)$. In the case that $\mu(E)$ is finite, we required that the series $\sum_{n \in N} \mu(E_n)$ converges absolutely to $\mu(E)$.
%		\item  A \textit{measure} is a signed measure that does not take negative values.
%	\end{enumerate}
%\end{defn}
%\begin{remark}
%	The triple $(\mathscr{X}, \mathscr{F}, \mu)$ is called a signed measure space or measure space as appropriate. The sets of all signed measures and measures on $(\mathscr{X}, \mathscr{F})$ are denoted $\mathscr{M}_{\pm}(\mathscr{X}, \mathscr{F})$ and $\mathscr{M}_{+}(\mathscr{X}, \mathscr{F})$ respectively.
%\end{remark}
This, brings up a new definition for the elements of space holding a property. 
\begin{defn}
	Let $(\mathscr{X}, \mathscr{F}, \mu)$ be a measure space.
	\begin{enumerate}[i.]
		\item If $N \subseteq \mathscr{X}$ is a subset of a measurable set $E \in \mathscr{F}$ such that $\mu(E) = 0$, then $N$ is called
a $\mu$-null set.
		\item If the set of $x \in \mathscr{X}$ for which some property $P(x)$ does not hold is $\mu$-null, then $P$ is said
to hold $\mu$-almost everywhere (or, when $\mu$ is a probability measure, $\mu$-almost surely).
	\end{enumerate}
\end{defn}
Moreover, we need a more precise definition of the notion of integration in this new space.
\begin{defn}
	Let $(\mathscr{X}, \mathscr{F})$ and $(\mathscr{Y}, \mathscr{G})$ be measurable spaces. A function $f : \mathscr{X} \to \mathscr{Y}$
generates a $\sigma-algebra$ on $\mathscr{X}$ by
	\begin{equation*}
	\sigma(f) := \sigma({[f \in E] | E \in \mathscr{G}}) ,
	\end{equation*}
	and $f$ is called a \textit{measurable function} if $\sigma(f) \subseteq \mathscr{F}$. That is, $f$ is measurable if the pre-image
$f^{-1}(E)$ of every $\mathscr{G}$-measurable subset $E$ of $Y$ is an $\mathscr{F}$-measurable subset of $\mathscr{X}$. A measurable
function whose domain is a probability space is usually called a \textit{random variable}.
\end{defn}
\subsection{Lebesgue Integration}
The integration of a measurable function with respect to a (signed or non-negative) measure is referred to as \textit{Lebesgue integration}. One can guess that it extends the simple Riemann integral of functions of a single real variable, can handle worse singularities than the Riemann integral.\\
The Lebesgue integral can define in three steps.\\
\textbf{Step i.} Calculating Lebesgue integral over simple functions.
\begin{defn}
	Let $(\mathscr{X}, \mathscr{F}, \mu)$ be a measure space. The indicator function $\mathbb{I}_E$ of a set
$E \in \mathscr{F}$ is the measurable function defined by
	\begin{equation*}
		\mathbb{I}_E := 
		\begin{cases}
		1,& \text{if } x \in E\\
		0,& \text{if} x \notin E.
		\end{cases}
	\end{equation*}
	A function $f : \mathscr{X} \to \mathbb{K}$ is called simple if
	\begin{equation*}
		f = \sum_{i=1}^{n} \alpha_i \mathbb{I}_{E_{i}}
	\end{equation*}
	for some scalars $\alpha_1, \dots ,\alpha_n \in \mathbb{K}$ and some pairwise disjoint measurable sets $E_1, \dots, E_n \in \mathscr{F}$
with $\mu(E_i)$ finite for $i = 1, \dots, n$. The Lebesgue integral of a simple function $f := \sum_{i=1}^{n} \alpha_i \mathbb{I}_{E_{i}}
$ is defined to be
	\begin{equation*}
		\int_{\mathscr{X}} f \text{d}\mu := \sum_{i=1}^{n} \alpha_i \mu(E_i).
	\end{equation*}
\end{defn}
\textbf{Step ii.} Calculating the integral of non-negative measurable functions using simple functions.
\begin{defn}
	Let $(\mathscr{X}, \mathscr{F}, \mu)$ be a measure space and let $f :  \mathscr{X} \to [0, + \infty]$ be a measurable
function. The Lebesgue integral of $f$ is defined to be
	\begin{equation*}
	\int_{\mathscr{X}} f \text{d}\mu := \sup \left\{ \int_{\mathscr{X}}\phi \text{d}\mu \middle\vert 
	\begin{aligned}
		& \phi : \mathscr{X} \to \mathbb{R} \text{ is a simple function, and} \\
		& 0 \leq \varphi(x) \leq f(x) \text{ for } \mu\text{-almost all } x \in \mathscr{X}
	\end{aligned}
	  \right \} \\
	\end{equation*}
\end{defn} 
\textbf{Step iii.} The integral of a real- or complex-valued function is defined through integration of
positive and negative real and imaginary parts, with care being taken to avoid the undefined
expression `$\infty - \infty$'
\begin{defn}
	Let $(\mathscr{X}, \mathscr{F}, \mu)$ be a measure space and let $f : \mathscr{X} \to \mathbb{R}$ be a measurable
function. The Lebesgue integral of $f$ is defined to be
	\begin{equation*}
		\int_{\mathscr{X}}f\text{d}\mu := \int_{\mathscr{X}}f_{+}\text{d}\mu + \int_{\mathscr{X}}f_{-}\text{d}\mu
	\end{equation*}
	provided that at least one of the integrals on the right-hand side is finite. The integral of a
complex-valued measurable function $f : \mathscr{X} \to \mathbb{C}$ is defined to be
	\begin{equation*}
		\int_{\mathscr{X}}f\text{d}\mu := \int_{\mathscr{X}}(\text{Re}f)\text{d}\mu + \int_{\mathscr{X}}(\text{Im}f)\text{d}\mu.
	\end{equation*}
\end{defn}
It is worthy to define the spaces of Lebesgue integrable functions which are ubiquitous in analysis.
\begin{defn}
	Let $(\mathscr{X}, \mathscr{F}, \mu)$ be a measure space. For $1 \leq p \leq \infty$, the $L^p$ space (or
Lebesgue space) is defined by
	\begin{equation*}
		L^p(\mathscr{X}, \mu; \mathbb{K}) := \{f : \mathscr{X} \to \mathbb{K} \vert f \text{ is measurable and } \norm{f}_{L^p(\mu)} \text{ is finite}\}.
	\end{equation*}
	For $1 \leq p \leq \infty$, the norm is defined by the integral expression
	\begin{equation}
		\norm{f}_{L^p(\mu)} := \bigg( \int_{\mathscr{X}}\vert f(x) \vert^p\text{d}\mu(x) \bigg)^{1/p}.
	\end{equation}
	To be more precise, $	L^p(\mathscr{X}, \mu; \mathbb{K})$ is the set of equivalence classes of such functions, where
functions that differ only on a set of $\mu$-measure zero are identified.
\end{defn}
%\begin{exmp}
%	A particular important case corresponds to taking $p=2$, then the inner product would be
%	\begin{equation*}
%	\innerProduct{u}{v} := \big( \int_{\Omega} \abs{u(x)}^2 \text{d}x \big)^{1/2}.
%	\end{equation*}
%	Clearly, $\norm{u}_{L^2(\Omega)} = \innerProduct{u}{u}^{1/2}$.
%\end{exmp}
\subsection{Sobolev Spaces}
\begin{defn}
	A \textit{multi-index} $\alpha$ is an $n$-tuple $\alpha = (\alpha_1, \dots, \alpha_n)$, used to concisely
denote the partial differential operator
	\begin{equation}
		D^\alpha (u) := \frac{\text{d}^{\abs{\alpha}}}{\text{d}x_{1}^{\alpha_1}\dots\text{d}_{x}^{\alpha_n}} (u).
	\end{equation}
	We define $ \abs{\alpha} = \alpha_1 + \dots + \alpha_n $ to be the \textit{degree} of $\alpha$.
\end{defn}
\begin{defn}
	For two multi-indices $\alpha, \beta$ we define the following:
	\begin{enumerate}[i.]
		\item $\alpha \leq \beta$ if $\alpha_i \leq \beta_i$ , for all $1 \leq i \leq n$
		\item If $\alpha \leq \beta$, we define $\alpha - \beta := \gamma$, where $\gamma = (\alpha_1-\beta_1, \dots,\alpha_n - \beta_n)$
		\item $\alpha! = \alpha_1!\dots\alpha_n!$
	\end{enumerate}
\end{defn}
\begin{notation}
	For the remainder of this subsection, let $\Omega$ be a bounded open subset of $\mathbb{R}^n$. In general, $\Omega$ would be the domain of the PDE which we want to solve.
\end{notation}
\begin{defn}
	\textbf{Weak Derivative}. Let $\Omega \subset \mathbb{R}^n$, $u, v \in L^p(\Omega)$, and $\alpha$ be a multi-index. Then $v$ is the weak $\alpha$-th partial derivative of $u$ if
	\begin{equation*}
	\int_{\Omega} uD^\alpha\phi \text{d}x = (-1)^{\abs{\alpha}}\int_{\Omega}v\phi\text{d}x
	\end{equation*}
	for all $\phi \in \mathscr{C}_{0}^{\infty}$.
\end{defn}
\begin{defn}
	Let $1\leq p \leq \infty$ and $k$ be a non-negative integer. The \textit{Sobolev Space} $\mathscr{W}^{k,p}(\Omega)$ consists of all functions $u : \Omega \to \mathbb{R}, u \in L^{p}_{loc} (\Omega)$ such that each weak
derivative $D^{\alpha}u$ with $\abs{\alpha}\leq k$ exists and belongs to $L^p$. That is,
	\begin{equation}
		\mathscr{W}^{k,p}(\Omega) = \{u \in L^{p}_{loc}(\Omega) \vert \norm{D^{\alpha}u}_{L^p(\Omega)}, \forall \abs{\alpha}\leq k\}
	\end{equation}
	Similarly, the space $\mathscr{W}^{k,p}_{loc}(\Omega)$
consists of the functions $u$ as above for which $D^\alpha u$ with $\abs{\alpha} \leq k$ exists and belongs to $L^{p}_{loc}(\mathscr{V})$, where $\mathscr{V}$ is an arbitrary compact subset of $\Omega$.
\end{defn}
\begin{remark}
	Note that $\mathscr{W}^{k,p}(\Omega) = \{ u\in L^{p}_{\Omega} \text{ | } D^\alpha u \in L^{p}_{\Omega}\text{, } \abs{\alpha} \leq k\}$.
\end{remark}
\begin{thm}
	Assume $u, v \in \mathscr{W}^{k,p}(\Omega)$,
$\abs{\alpha} \leq k$. Then
	\begin{enumerate}[i.]
		\item $D^{\alpha}(\Omega) \in \mathscr{W}^{k-\abs{\alpha},p}(\Omega)$ and $D^{\beta}(D^{\alpha}u) = D^\alpha(D^\beta u) = D^{\alpha+\beta}u$ for all multi-indices $\alpha, \beta$ satisfying $\abs{\alpha} + \abs{\beta}\leq k$.
		\item  For each $\lambda, \mu \in \mathbb{R}$, $\lambda u + \mu v \in \mathscr{W}^{k,p}(\Omega)$ and $D^\alpha(\lambda u + \mu v) = \lambda D^\alpha u + \mu D^\alpha v$,
$\abs{\alpha} \leq k.$
	\end{enumerate}
\end{thm}
\begin{defn}
	Norm on $u \in  \mathscr{W}^{k,p}(\Omega)$ can be defined by
	\begin{equation*}
		\norm{u}_{\mathscr{W}^{k,p}(\Omega)} := 
		\begin{cases}
			\left( \sum_{\abs{\alpha}\leq k} \int_{\Omega} \abs{D^\alpha u}^p \text{d}x \right)^{1/p},& \text{if } 1 \leq p \leq \infty\\
			\sum_{\abs{\alpha}\leq k} \esssup{\Omega}{\abs{D^\alpha u}},& \text{if} p = \infty.
		\end{cases}
	\end{equation*}
\end{defn}
The following theorem states that Sobolev spaces are complete.
\begin{thm}
	Let $\Omega \in \mathbb{R}^n$ be an open bounded set
and $1 \leq p \leq \infty$. Then
	\begin{enumerate}[i.]
		\item the space $\mathscr{W}^{k,p}(\Omega)$ is a Banach space with respect to the norm $\norm{\cdot}_{\mathscr{W}^{k,p}}$
		\item the space $H^1(\Omega) := \mathscr{W}^{1,2}(\Omega)$ is a Hilbert space with inner product
		\begin{equation*}
			\langle u,v \rangle := \int_{\Omega} uv \text{d}x + \sum_{i=1}^{N}\int_{\Omega} \frac{\partial u}{\partial x_i} \frac{\partial v}{\partial x_i} \text{d}x.
		\end{equation*}
	\end{enumerate}
\end{thm}
\begin{lemma}
	Given $u \in \mathscr{W}^{k,p}(\Omega)$ and $v \in \mathscr{C}_{0}^{\infty} (\Omega)$, $uv \in \mathscr{W}^{k,p}(\Omega)$.
\end{lemma}
Up to this point, the goal was to expand the vocabulary to use it in theoretic justifications of the NN model. Like understanding the universal approximation theorem, or showing the convergence of NN model. Here, it is fit to take a closer look at the FEM, and through that, pin pointing the bottlenecks of it, if any. 
\section{Finite Element Method}
\label{sec:finite_element_method}
This section would review FEM. The goal is to highlight the limitations of this method. For interested readers, we suggest reading FE resources \cite{suli2012lecture, johnson2012numerical}. Moreover, one may be interested in using parallel programming to enhance FEMs performance \cite{douglas2003tutorial}. However, one needs to question the nature of the method itself to find a way to improve it.
\begin{notation}
	Let $\Omega$ be an open set in $\mathbb{R}^n$ and let $k \in \mathbb{N}$. Denote the set of all continuous real-valued functions by $\mathscr{C}^k(\Omega)$. These functions defined on $\Omega$ such that $D^\alpha u$ is continuous on $\Omega$ for all $\alpha = (\alpha_1 , \dots, \alpha_n)$ with $\abs{\alpha} \leq k$. Assuming that $\Omega$ is a bounded open set, $\mathscr{C}^k (\Omega)$ will denote the set of all $u$ in $\mathscr{C}^k (\Omega)$ such that $D^\alpha u$ can be extended from $\Omega$ to a continuous function on $\bar{\Omega}$, the closure of the set $\Omega$, for all $\alpha = (\alpha_1 , \dots, \alpha^n)$, $\abs{\alpha} \leq k$.
 $\mathscr{C}^k (\bar{\Omega})$ can be equipped with the norm
	\begin{equation*}
		\norm{u}_{\mathscr{C}^k(\bar{\Omega})} := \sum_{\abs{\alpha} \leq k} \sup_{x\in\Omega} \abs{D^\alpha u(x)}.
	\end{equation*}
	In particular when $k = 0$ we shall write $\mathscr{C}(\bar{\Omega})$ instead of $\mathscr{C}^{0}(\bar{\Omega})$ to denote the set of all continuous functions defined on $\bar{\Omega}$; in this case,
	\begin{equation*}
		\norm{u}_{\mathscr{C}(\bar{\Omega})} = \sup_{x\in\Omega} \abs{u(x)} = \max_{x\in\bar{\Omega}} \abs{u(x)}.
	\end{equation*}
	Similarly, if $k = 1$,
	\begin{equation*}
		\norm{u}_{\mathscr{C}(\bar{\Omega})} = \sum_{\abs{\alpha}\leq 1} \sup \abs{D^\alpha u(x)} = \sup_{x\in\Omega} \abs{u(x)} + \sum_{j=1}^{n} \sup_{x\in\Omega} \abs{\frac{\partial u}{\partial x_j}(x)}.
	\end{equation*}
\end{notation}
\begin{defn}
	The support of a continuous function $u$ on an open set $\Omega \subset \mathbb{R}^n$ is defined as the closure in $\Omega$ of the set ${x \in \Omega : u(x) \neq 0}$. We shall write \textit{supp $u$} for the support of $u$, which is the smallest closed subset of $\Omega$ such that $u = 0$ in $\Omega\setminus\text{supp } u$.
\end{defn}
Denote the set of all $u$ contained in $\mathscr{C}^{k}(\Omega)$ whose support is a bounded subset of $\Omega$ by $\mathscr{C}_{0}^{k}(\Omega)$. Let $\mathscr{C}_{0}^{\infty} = \cap_{k \geq 0} \mathscr{C}_{0}^{k}(\Omega)$.
%\begin{lemma}
%	\textit{(The Cauchy-Schwarz inequality)}. Let $u$ and $v$ belong to $L^2_{\Omega}$; then $uv \in L^1_{\Omega}$ and 
%	\begin{equation*}
%		\abs{\innerProduct{u}{v}} \leq \norm{u}_{L^2_{\Omega}}  \norm{v}_{L^2_{\Omega}}.
%	\end{equation*}
%\end{lemma}
%\begin{corollary}
%	\textit{(The triangle inequality)}. Let $u$ and $v$ belong to $L^2_{\Omega}$; then $u+v \in L^2_{\Omega}$, and
%	\begin{equation*}
%		\norm{u+v}_{L^2_{\Omega}} \leq \norm{u}_{L^2_{\Omega}} + \norm{v}_{L^2_{\Omega}}.
%	\end{equation*} 
%\end{corollary}
%\begin{defn}
%	Let $\mathscr{X}$ be a linear space with norm $\norm{\cdot}_{\mathscr{X}}$. $\mathscr{X}$ is called a \textit{Banach space} if, whenever $\{u_m\}_{m=1}^{\infty}$ is a sequence of elements of $\mathscr{X}$ such that
%	\begin{equation*}
%		\lim_{n, m\to \infty} \norm{u_n - u_m}_{\mathscr{X}} = 0,
%	\end{equation*}
%	there exists $u\in \mathscr{X}$ such that $\lim_{m\to \infty} \norm{u - u_m}_{\mathscr{X}} = 0$ (i.e. the sequence $\{u_m\}_{m=1}^{\infty}$ converges to $u$ in $\mathscr{X}$). Furthermore, such a sequence is called a \textit{Cauchy sequence}.
%\end{defn}
%\begin{remark}
%	Every Banach space equipped with an inner product is a \textit{Hilbert} space. Note that if $\mathscr{H}$ is Hilbert and $u \in \mathscr{H}$, then $\norm{u}_{\mathscr{H}} = \innerProduct{u}{u}^{1/2}$
%\end{remark}
%\begin{remark}
%	The space $L^p_{\Omega}$ with $p \in [1, \infty]$ is a Banach space. In particular, $L^2_{\Omega}$ is a Hilbert space.
%\end{remark}
\begin{remark}
	Assuming that the reader is familiar with Hilbert spaces, the Hilbert space $\mathscr{H}_{0}^{1}(\Omega)$ has significant importance in our context. It defines as follows:
	\begin{equation*}
	\mathscr{H}_{0}^{1}(\Omega) = \biggl\{ u\in L^2(\Omega) \vert \frac{\partial u}{\partial x_i} \in L^2(\Omega)\text{, } i=1,\dots,n\text{, } u=0 \text{ on } \partial\Omega \biggr\}.
	\end{equation*}
\end{remark}
\subsection{Weak Solutions to Elliptic Problems}
Let $\Omega$ be a bounded open set in $\mathbb{R}^n$, and consider the linear second-order partial differential equation
\begin{eqnarray}\label{elliptic_equation}
	-\sum_{i,j=1}^{n} \frac{\partial}{\partial x_j} \bigg(a_{i,j}(x)\frac{\partial u}{\partial x_i}\bigg) + \sum_{i=1}^{n} b_{i}(X)\frac{\partial u}{\partial x_i} + c(x)u = f(x) \text{,     } x \in \Omega \\
	\label{bc_of_elliptic_equation}
	u = 0 \text{ on } \partial\Omega
\end{eqnarray}
where $\partial\Omega$ indicates the boundary of the domain $\Omega$,
\begin{equation*}
a_{i,j} \in \mathscr{C}^1(\bar{\Omega}), \quad i,j = 1,\dots, n; \quad 
b_i \in \mathscr{C}(\bar{\Omega}), \quad i = 1,\dots, n;\quad
c \in \mathscr{C}(\bar{\Omega}), \quad f\in\mathscr{C}(\bar{\Omega})
\end{equation*}
and 
\begin{equation}\label{uniform_ellipticity}
	\sum_{i,j=1}^{n}a_{i,j}(x)\zeta_i \zeta_j \geq \tilde{c} \sum_{i=1}^{n} \zeta_i^2, \quad \forall\zeta = (\zeta_1,\dots, \zeta_n)\in \mathbb{R}^n, \quad  x\in \bar{\Omega},
\end{equation}
here $\tilde{c}$ is a positive constant independent of $x$ and $\zeta$. The condition in (\ref{uniform_ellipticity}) is usually referred to as \textit{uniform ellipticity} and (\ref{elliptic_equation}) is called an elliptic equation. Here, as \ref{bc_of_elliptic_equation} stated, we took the boundary condition as Dirichlet boundary condition for simplicity.\\
\begin{defn}
	A \textbf{classical solution} of (\ref{elliptic_equation}) is any function $u\in\mathscr{C}^2(\Omega)\cap\mathscr{C}(\bar{\Omega})$ which satisfies (\ref{elliptic_equation}) and (\ref{bc_of_elliptic_equation}). 
\end{defn}
According to the theory of partial differential equations, the equation \eqref{elliptic_equation} with boundary condition \eqref{bc_of_elliptic_equation} has a unique classical solution, provided that $a_{i,j} , b_i , c, f$ and $\partial\Omega$ are sufficiently smooth. %However, in many applications one has to consider equations where these smoothness requirements are violated, and for such problems the classical theory is inappropriate. 
%\begin{exmp}
%	 Poisson’s equation with zero Dirichlet boundary
%condition on $\Omega = (−1, 1)^n$ in $\mathbb{R}^n$:
%	 \begin{equation*} \tag{$\star$}
%	 	\begin{cases}
%	 	-\Delta u = \text{sgn}(\frac{1}{2}-\abs{x}), & x\in\Omega,\\
%	 	u = 0, & x\in\partial\Omega,
%	 	\end{cases}
%	 \end{equation*}
%	 This problem does not have a classical solution, $u\in\mathscr{C}^2(\Omega)\cap\mathscr{C}(\bar{\Omega})$, for otherwise $\Delta u$ would be a continuous function on $\Omega$, which is not possible because $\text{sgn}(\frac{1}{2} − \abs{x})$
%is not continuous on $\Omega$.
%\end{exmp}
In order to overcome the limitations of the classical theory and deal with partial differential equations with “non-smooth” data, the idea is to generalise the notion of solution by weakening the differentiability requirements on $u$.
\begin{defn}
	Let $a_{i, j} \in L^{\infty}(\Omega)$, $i, j = 1, \dots, n$, $b_i \in L^{\infty}(\Omega)$, $i = 1, \dots$, $n, c \in L^{\infty}(\Omega)$, and let $f \in L^2(\Omega)$. A function $u\in\mathscr{H}_{0}^{1}(\Omega)$ satisfying
	\begin{equation}
	\label{weak_sulotion_definition}
	\sum_{i,j=1}^{n} \int_{\Omega} a_{i, j}(x) \frac{\partial u}{\partial x_i} \frac{\partial v}{\partial x_j} \text{d}x + \sum_{i=1}^{n} \int_{\Omega} b_{i}(x) \frac{\partial u}{\partial x_i} v \text{d}x + \int_{\Omega} c(x)uv \text{d} x = \int_{\Omega} f(x)v(x) \text{d}x \text{,    } \forall v \in \mathscr{H}_{0}^{1}(\Omega)
	\end{equation}
	is called a \textbf{weak solution} of \eqref{elliptic_equation}, \eqref{bc_of_elliptic_equation}. All partial derivatives in \ref{weak_sulotion_definition} should be
understood as weak derivatives.
\end{defn}
Clearly, if $u$ is a classical solution of \eqref{elliptic_equation}, \eqref{bc_of_elliptic_equation}, then it is also a weak solution of \eqref{elliptic_equation}, \eqref{bc_of_elliptic_equation}. However, the converse is not true. If \eqref{elliptic_equation}, \ref{bc_of_elliptic_equation} has a weak solution, this may not be smooth enough to be a classical solution. 
%Before giving an example of such a situation, it would be convenient to have a tool to show the existence of a unique solution to the PDEs.
%\begin{thm}
%	\label{lax_milgram_lemma}
%	\textit{(Lax \& Milgram theorem)}. Suppose that $\mathscr{V}$ is a real Hilbert space equipped with norm $\norm{\cdot}_{\mathscr{V}}$. Let $a(\cdot, \cdot)$ be a bilinear functional on $\mathscr{V}\times\mathscr{V}$ such that:
%	\begin{enumerate} [i.]
%		\item $\exists c_0 > 0 \text{ s.t. } \forall v \in \mathscr{V}\text{ : }a(v, v)\geq c_0 \norm{v}_{\mathscr{V}}^{2}$,
%		\item $\exists c_1 > 0 \text{ s.t. } \forall c,w \in \mathscr{V}\text{ : } \abs{a(w, v)} \leq c_1 \norm{w}_{\mathscr{V}}\norm{v}_{\mathscr{V}}$, 
%		\item let $l(\cdot)$ be a linear functional on $\mathscr{V}$ such that : $\exists c_2 > 0$ s.t. $\forall v\in \mathscr{V}$ : $\abs{l(v)}\leq c_2 \norm{v}_{\mathscr{V}}$.
%	\end{enumerate}
%	Then, there exists a unique $u \in \mathscr{V}$ such that 
%	\begin{equation*}
%			a(u, v) = l(v), \quad \forall v \in \mathscr{V}.
%	\end{equation*}
%\end{thm}
%\begin{remark}
%	If
%	\begin{equation*}
%		c(x) - \frac{1}{2} \sum_{i=1}^{n} \frac{\partial b_i}{\partial x_i} \geq 0 \text{,     }x\in \bar{\Omega},
%	\end{equation*}
%	then has a unique solution $u\in\mathscr{H}_{0}^{1}$.
%\end{remark}
The first step in the construction of a FEM for an elliptic boundary value problem, (e.g. (\ref{elliptic_equation}), (\ref{bc_of_elliptic_equation})) is to convert it into its weak formulation
\begin{equation*}
	\text{find } u \in \mathscr{V}\text{ such that }a(u, v) = l(v)\text{ }\forall v \in \mathscr{V}.  \tag{P}
\end{equation*}
where $\mathscr{V}$ is the solution space (e.g. $\mathscr{H}_{0}^{1}(\Omega)$ for the homogeneous Dirichlet boundary value problem), $a(\cdot, \cdot)$ is a bilinear functional on $\mathscr{V}\times\mathscr{V}$, and $l(\cdot)$ is a linear functional on $\mathscr{V}$.
The second step in the construction is to replace $\mathscr{V}$ in (P) with a finite-dimensional subspace $\mathscr{V}_h \subset \mathscr{V}$, which consists of continuous piecewise polynomial functions of a fixed degree associated with a subdivision of the computational domain; then consider the following approximation of (P):
\begin{equation*}\tag{$P_{h}$}
	\text{find } u_h \in \mathscr{V}_h\text{ such that } a(u_h, v_h) = l(v_h) \text{ } \forall v_h \in \mathscr{V}_h .  
\end{equation*}
Suppose, for example, that
\begin{equation*}
\text{dim }\mathscr{V}_h = N(h) \text{ and } \mathscr{V}_h = \text{span} \{ \phi_1, \dots, \phi_{N(h)} \}
\end{equation*}
where the (linearly independent) basis functions $\phi_i, i=1,\dots,N(h)$, have ``small'' support. Expressing the approximate solution $u_h$ in terms of the basis functions, $\phi_i$, we can write
\begin{equation*}
\tag{$\star\star$}
u_h (x) = \sum_{i=1}^{N(h)} U_i \phi_{i}(x),
\end{equation*}
where $U_i, i = 1, \dots, N(h)$, are to be determined. Thus, ($P_h$) can be written as follows:
\begin{equation*}
\tag{$P_h^{'}$}
\text{find } (U_1, \dots, U_{N(h)}) \in \mathbb{R}^{N(h)} \text{ s.t. } \sum_{i=1}^{N(h)} a(\phi_i, \phi_j) U_i = l(\phi_i)\text{,     }j=1,\dots, N(h).
\end{equation*}
This is a system of linear equations for $U = (U_1 , \dots, U_{N(h)})^T$, with the matrix of the system $A = (a(\phi_i, \phi_j))$ of size $N(h)\times N(h)$. Because the $\phi_i\text{'s}$ have small support, $a(\phi_i, \phi_j) = 0$ for most pairs of $i$ and $j$, so the matrix $A$ is sparse (in the sense that most of its entries are equal to 0); this property is crucial from the point
of efficient solution - in particular, fast iterative methods are available for sparse linear systems. Once ($P_h{'}$) has been solved for $U = (U_1 , \dots, U_{N(h)})^T$, the expansion ($\star\star$) provides the required approximation to $u$. The matrix A is called the stiffness matrix. It is clear that to solve the PDE, one has to construct this stiffness matrix, which in turns impose the necessity of calculating basis functions $\phi_i, i=1,\dots,N(h)$ on nodal points. Thus, time needed to computations grows as the number of the nodal points grows. In a sense, if one can solve the PDE without the need to compute the basis function values over nodal point, he can improve computational cost.
\subsection{The Self-Adjoint Elliptic Problem}
In the special case, when the boundary value problem is self-adjoint, i.e.,
\begin{equation*}
a_{i, j} = a_{j, i} \text{,     }i,j=1,\dots,n\text{,    }x\in\bar{\Omega},
\end{equation*}
and
\begin{equation*}
b_i \equiv 0 \text{,     }i=1,\dots,n \text{,  }x\in\bar\Omega,
\end{equation*}
the bilinear functional $a(\cdot, \cdot)$ is symmetric in the sense that
\begin{equation*}
	a(v,w) = a(w,v), \quad \forall v,w \in\mathscr{H}_{0}^{1}(\Omega).
\end{equation*}
Thus, consider
\begin{equation}
\label{self_adjoint_elliptic_equation}
-\sum_{i,j=1}^{n} \frac{\partial}{\partial x_j} \bigg(a_{i,j}(x)\frac{\partial u}{\partial x_i}\bigg) + c(x)u = f(x) \text{,     } x \in \Omega \\
u = 0 \text{ on } \partial\Omega
\end{equation}
with $a_{i,j}(x)$ satisfying the ellipticity condition (\ref{uniform_ellipticity}); $a_{i,j}=a_{j,i}$, $c(x)\geq0$, $x\in\bar{\Omega}$. 
If we define the quadratic functional $J:\mathscr{H}_{0}^{1}(\Omega) \to \mathbb{R}$ by
\begin{equation*}
	J(v) = \frac{1}{2} a(v, v) - l(v) \text{,     }v\in\mathscr{H}_{0}^{1}(\Omega).
\end{equation*}
then we can restate (\ref{self_adjoint_elliptic_equation}) as a minimization problem. 
\begin{lemma}
	Let $u\in \mathscr{H}_{0}^{1}(\Omega)$ be the (unique) weak solution to (P) with $\mathscr{V} = \mathscr{H}_{0}^{1}(\Omega)$ and suppose that $a(\cdot, \cdot)$ is a symmetric bilinear functional on $\mathscr{H}_{0}^{1}(\Omega)$; then $u$ is the unique minimizer of $J(\cdot)$ over $\mathscr{H}_{0}^{1}(\Omega)$.
\end{lemma}
\begin{lemma}
	Let $u\in \mathscr{H}_{0}^{1}(\Omega)$ minimizes $J(\cdot)$ over $\mathscr{H}_{0}^{1}(\Omega)$; then $u$ is the (unique) solution of the problem (P) with $\mathscr{V} = \mathscr{H}_{0}^{1}(\Omega)$. The problem (P) is called the \textbf{Euler-Lagrange equation} for this minimization problem.
\end{lemma}
As we would see in (\ref{universal_approximation_theorem}), convexity is very important in our context. Indeed, it is easy to show that $J(\cdot)$ is convex, i.e.
\begin{equation*}
J((1-\theta)v + \theta w) \leq (1-\theta) J(v) + \theta J(w) \text{,     }\forall \theta \in [0,1], \quad\forall v,w \in \mathscr{H}_{0}^{1}(\Omega),
\end{equation*}
which follows from the identity
\begin{equation*}
(1-\theta) J(v) + \theta J(w) = J((1-\theta)v + \theta w) + \frac{1}{2} \theta(1-\theta) a(v-w,v-w)
\end{equation*}
and the fact that $ a(v-w,v-w) \geq 0$. Also, note that if $u$ minimises $J(\cdot)$ then $J(\cdot)$ has a stationary point at $u$.\\
One can see that following problems are equivalent:
\begin{equation*}
\tag{W}
\text{find }u\in  \mathscr{H}_{0}^{1}(\Omega) \text{ such that } a(u,v) = l(v) \text{    }\forall v \in \mathscr{H}_{0}^{1}(\Omega),
\end{equation*}
\begin{equation*}
\tag{M}
\text{find }u\in  \mathscr{H}_{0}^{1}(\Omega) \text{ such that } J(u) \leq J(v) \text{    }\forall v \in \mathscr{H}_{0}^{1}(\Omega).
\end{equation*}
Given that $\mathscr{V_h}$ is a certain finite-dimensional subspace of $\mathscr{H}_{0}^{1}(\Omega)$ which consists of continuous piecewise polynomiyals of a fixed degree, then the finite element approximation of (W) and (M) would be as follows respectively:
\begin{equation*}
\tag{$W_h$}
\text{find }u_h\in  \mathscr{V}_h \text{ such that } a(u_h,v_h) = l(v_h) \text{    }\forall v_h \in \mathscr{V}_h,
\end{equation*}
\begin{equation*}
\tag{$M_h$}
\text{find }u_h\in  \mathscr{V}_h \text{ such that } J(u_h) \leq J(v_h) \text{    }\forall v_h \in \mathscr{V}_h.
\end{equation*}
The problem of calculating basis functions over nodal points stays still. For enthusiastic readers, it is worth mentioning that the self adjoint elliptic problem, can be used as base for a NN model called Deep Ritz Method \cite{weinan2018deep}.% The key point is that the NN also try to solve a minimization problem. Then with choosing appropriate loss function, one can use of it's nature to solve the self-adjoint problem. 
\section{Neural Network}\label{sec:neural_network}
Before going any further, one has to show that the NN models can do approximation. Universal approximation theorem is our main tool for construction of neural networks which can be used to solve PDEs. It gives us information needed to ensure that our network would converge to the solution of our problem. It states that a feedforward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of $\mathbb{R}^n$, under mild assumptions on the activation function. George Cybenko in 1989 proved the theorem for \textit{sigmoid} activation functions \cite{cybenko1989approximation}. Later, in \cite{li1999alpha} it has been shown that the class of deep neural networks is a universal approximation if and only if the activation function is not polynomial. It had shown in 1991 by Kurt Hornik  that it is not the specific choice of the activation function, but rather the multilayer feedforward architecture itself which gives neural networks the potential of being universal approximators \cite{hofbauer1990adaptive}; the output units are always assumed to be linear. \\
\textit{feedforward networks} with a single hidden layer are universal approximators. However, the width of such networks has to be exponentially large. In 2017 Lu et al. proved universal approximation theorem for width-bounded deep neural networks \cite{lu2017expressive}. In particular, they showed that width $n+4$ networks with ReLU activation functions can approximate any \textit{Lebesgue integrable function} on $n$-dimensional input space with respect to $L^{1}$ distance if network depth is allowed to grow which can be achieved with \textit{Residual Networks} \cite{he2016deep, lin2018resnet}. They also showed the limited expressive power if the width is less than or equal to $n$. A later result of their work showed that ReLU networks with width $n+1$ is sufficient to approximate any continuous function of $n$-dimensional input variables \cite{hanin2017approximating}. The survey \cite{tikk2003survey} is a good starting point to read and enthusiastic readers may find \cite{kratsios2019universal} quite interesting.
\begin{thm}
	\label{universal_approximation_theorem}
	\textbf{(Universal Approximation Theorem)}.
	\begin{enumerate}[i.]
		\item \textbf{(Unbounded Width case)} Let $\varphi :\mathbb {R} \to \mathbb {R}$ be a non-constant, bounded, and continuous function (called the activation function). Let $I_{m}$ denote the $m$-dimensional unit hypercube $[0,1]^{m}$. The space of real-valued continuous functions on $I_m$ is denoted by  $C(I_{m})$. Then, given any $\varepsilon > 0$ and any function  $f\in C(I_{m})$, there exist an integer  $N$, real constants $v_{i},b_{i}\in \mathbb  {R}$ and real vectors $w_{i}\in \mathbb {R} ^{m}$ for $i=1,\dots ,N$, such that we may define	
		 \begin{equation*}
		  F(x)=\sum _{{i=1}}^{{N}}v_{i}\varphi \left(w_{i}^{T}x+b_{i}\right)
		 \end{equation*}
		as an approximate realization of the function $f$; that is,
		\begin{equation*}
		\abs{F(x) - f (x)}  < \varepsilon
		\end{equation*}
		for all $x\in I_{m}$. In other words, functions of the form $F(x)$ are dense in $C(I_{m})$.\\	
		This still holds when replacing $I_m$ with any compact subset of $\mathbb {R} ^{m}$.
		\item \textbf{(Bounded Width Case)} The universal approximation theorem for width-bounded networks can be expressed mathematically as follows\\
		For any Lebesgue-integrable function $f:\mathbb {R} ^{n}\rightarrow \mathbb {R}$ and any $\epsilon >0$, there exists a fully-connected ReLU network $\mathcal {A}$ with width $ d_{m}\leq {n+4}$, such that the function $F_{\mathcal {A}}$ represented by this network satisfies
		\begin{equation*}
		\int _{\mathbb {R} ^{n}}\left|f(x)-F_{\mathcal {A}}(x)\right|\mathrm {d} x<\epsilon.
		\end{equation*}
	\end{enumerate}
\end{thm}
Here is a list of some of important results and works from literature
\begin{enumerate}[i.]
	\item Multilayer feedforward networks are universal approximators \cite{hornik1989multilayer}.
	\item Neural network with unbounded activation functions is universal approximator \cite{sonoda2017neural}.
	\item Optimal approximation of piecewise smooth functions using deep ReLU neural networks \cite{petersen2018optimal}.
	\item On the approximation by neural networks with bounded number of neurons in hidden layers \cite{ismailov2014approximation}.
	\item Resnet with one-neuron hidden layers is a universal approximator \cite{lin2018resnet}.
\end{enumerate}
Another important tool for us which enables us to set an error bound for our network is residual connections \cite{he2016deep}.\\
Residual connections is the tool that provide the possibility to deepening the network as much as possible without overfitting. 