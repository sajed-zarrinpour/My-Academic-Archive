\chapter{Mathematical Models}
In this chapter, we will introduce two PDEs which we want to solve as an example. To maintain generality we choose one linear inhomogeneous elliptic equation which is the effective conductance in a non-homogeneous media, and one nonlinear inhomogeneous Schr\"{o}dinger equation with random background potential.
\section{Effective Coefficients for Inhomogeneous Elliptic Equation}\label{section_ECIM}
Let 
\begin{equation}
	\label{def:coefficient_field_for_effective_conductance}
	\mathscr{A} = \{a\in L^{\infty}([0,1]^d) | \lambda_{1} \geq a(x) \geq \lambda_{0} > 0 \},
\end{equation}
for some fixed constants $\lambda_{0}$ and $\lambda_{1}$. Then the effective conductance/coefficient in a non-homogeneous media can be expressed as a functional $A_{\eff} : \mathscr{A} \to \mathbb{R}$ defined by
\begin{equation}
	A_{\eff}(a) = \min_{u(x)} \int_{[0,1]^d} a(x) ||\nabla u(x) + \xi||_{2}^{2} \dx
	\label{eq:effective_conductance_variational_form},
\end{equation}
where $\xi$ is a fix direction in $\mathbb{R}^d$ with $||\xi||_{2} = 1$ ($||.||_{2}$ is the Euclidean norm).
For $x\in [0,1]^{d}$ the minimizer $u_{a}(x)$ of the variational problem \eqref{eq:effective_conductance_variational_form} would satisfy the following elliptic partial differential equation: 
\begin{equation}
	-\nabla \cdot (a(x)(\nabla u(x)+\xi)) = 0 
	\label{eq:effective_conductance_pde_form}.
\end{equation}
To confirm that \eqref{eq:effective_conductance_pde_form} and \eqref{eq:effective_conductance_variational_form} are equivalent we define $\tilde{J}(\lambda)$ as $\tilde{J}(\lambda) = J(u_{a}+\lambda\phi)$ where $J(u) = \int_{[0,1]^d} a(x) ||\nabla u(x) + \xi||_{2}^{2} \dx $ for $\lambda \in \mathbb{R}$ and $\phi \in C_{0}^{\infty}([0,1]^{d})$. Then 
\begin{equation}
	\begin{aligned}
		\tilde{J}(\lambda) = &\int_{[0,1]^d} a(x) ||\nabla u_{a}(x)+ \lambda\nabla\phi + \xi||_{2}^{2} \dx \\
		= &\int_{[0,1]^d} a(x) [<\nabla u_{a}(x)+\xi,\nabla u_{a}(x)+\xi> + 2\lambda<\nabla u_{a}(x)+\xi, \nabla\phi> + \lambda^{2} <\nabla\phi,\nabla\phi>] \dx.
	\end{aligned}\label{eq:Jtilda}
\end{equation}
The idea is to find the minimum of $\tilde{J}(\lambda)$ using its derivatives because we defined $\tilde{J}(\lambda)$ in a way that it has the same minimum as $A_{\eff}$. Lets calculate the derivative of $\tilde{J}(\lambda)$ 
\begin{equation}
	\tilde{J}'(\lambda)= \int_{[0,1]} a(x) [2<\nabla u_{a}(x)+\xi, \nabla\phi> + 2\lambda <\nabla\phi,\nabla\phi>] \dx.
\end{equation}
Due to the definition of $\tilde{J}'(\lambda)$, one can see that $\tilde{J}'(0) = 0$, so
\begin{align}
	\tilde{J}'(0) &= 2\int_{[0,1]} a(x) (\nabla u_{a}(x)+\xi) \cdot \nabla\phi \dx\\
	&= 2 [a(x)(\nabla u_{a}(x)+\xi) \cdot \phi - \int_{[0,1]} \nabla \cdot(a(x) (\nabla u_{a}(x)+\xi))\cdot \phi \dx]\\ &= 0.
\end{align}
Since $\phi$ is a compact support function on $[0,1]$, then the $\phi = 0$ out of the boundary
\begin{equation}
	\tilde{J}'(0) = \int_{[0,1]} - \nabla \cdot(a(x) (\nabla u_{a}(x)+\xi))\cdot \phi \dx = 0.
\end{equation}
Furthermore, since $\phi \in c_{0}^{\infty}$ then for any $\psi \in L^{2}([0,1]^d)$ there is a sequence like $\{\phi_{i}\}_{i=0}^{\infty}$ which converges to $\psi$. Putting these together yields\\ $- \nabla \cdot(a(x) (\nabla u_{a}(x)+\xi))\aeeq0$.\\
%Now we can solve \eqref{eq:effective_conductance_pde_form} instead of \eqref{eq:effective_conductance_variational_form} using Finite Difference Method (FDM). To do that, lets begin by approximating the first and second derivatives by:
%%amrie.msrt
\begin{notation}
	To simplify finite difference notations in higher dimensions, suppose
	\begin{enumerate}[i.]
		\item $\{e_{k}\}_{k=1}^{d}$ is the the canonical basis in $\mathbb{R}^{d}$,
		\item $i \in {(i_1, \dots, i_d) \vert 1 \leq i_1, \dots, i_d \leq n}$. 
	\end{enumerate}
	Then $u_{i \pm e_k }$ denotes $u_{i_1, \dots, i_{k \pm 1}, \dots, i_d}$.
\end{notation}
With this notation, to write the finite difference schema one begins by approximating the first and second derivatives: 
\begin{equation}
	\frac{\partial}{\partial x_k} a(x) = \frac{a_{i+\frac{1}{2}e_{k}} - a_{i-\frac{1}{2}e_{k}}}{h}
\end{equation}
\begin{equation}
	\frac{\partial}{\partial x_k} u(x) = \frac{u_{i+e_{k}} - u_{i}}{h}
\end{equation}
\begin{equation}
	\frac{\partial^2}{\partial x_k^2} u(x) = \frac{u_{i+e_{k}} - 2 u_{i} + u_{i-e_{k}}}{h^2}
\end{equation}
where $a_{i+\frac{1}{2}e_{k}} = \frac{a_{i+e_{k}} + a_{i}}{2}$ and $a_{i-\frac{1}{2}e_{k}} = \frac{a_{i-e_{k}} + a_{i}}{2}$. Now Lets rewrite \eqref{eq:effective_conductance_pde_form} 
\begin{align*}
	-\nabla\cdot (a(x)(\nabla u(x)+\xi)) &= -\nabla\cdot (a(x)\nabla u(x) + a(x) \xi) \\ &= -\nabla a(x) \nabla u(x) - a(x) \nabla^2 u(x) - \nabla a(x)\xi \\ &= - \sum_{k=1}^{d} \frac{\partial}{\partial x_k} a(x) \frac{\partial}{\partial x_k} u(x) - \sum_{k=1}^{d} a(x) \frac{\partial^2}{\partial x_k^2} u(x) - \sum_{k=1}^{d} \frac{\partial}{\partial x_k} a(x) \xi_k \\ & =0.
\end{align*}
Next, we discretize the domain using a uniform grid with step size $h=\frac{1}{n}$ and grid points denoted by $x_i = i \times h$, where the multi-index $i\in\{(i_1,\cdots,i_d)| 1\leq i_1,\cdots,i_d\leq n\}$ and then we can discretize the former equatoin:
\begin{multline*}
	-\sum_{k=1}^{d} \frac{a_{i+\frac{1}{2}e_{k}} - a_{i-\frac{1}{2}e_{k}}}{h} \cdot \frac{u_{i+e_{k}} - u_{i}}{h} - \sum_{k=1}^{d} a_{i-\frac{1}{2} e_k}  \frac{u_{i+e_{k}} - 2 u_{i} + u_{i-e_{k}}}{h^2} - \sum_{k=1}^{d} \xi_k \frac{a_{i+\frac{1}{2}e_{k}} - a_{i-\frac{1}{2}e_{k}}}{h} \\
	= - \sum_{k=1}^{d} \frac{ a_{i+\frac{1}{2}e_{k}} u_{i+e_k} - a_{i+\frac{1}{2}e_{k}} u_i - a_{i-\frac{1}{2}e_{k}} u_{i+e_k} + a_{i-\frac{1}{2}e_{k}} u_i + a_{i-\frac{1}{2}e_{k}} u_{i+e_k} - 2 a_{i-\frac{1}{2}e_{k}} u_i + a_{i-\frac{1}{2}e_{k}} u_{i-e_k}}{h^2} \\ - \sum_{k=1}^{d} \xi_k \frac{a_{i+\frac{1}{2}e_{k}} - a_{i-\frac{1}{2}e_{k}}}{h} \\
	= - \sum_{k=1}^{d} \frac{ a_{i+\frac{1}{2}e_{k}} [u_{i+e_k} - u_i] -  a_{i-\frac{1}{2}e_{k}} [u_i - u_{i-e_k}]}{h^2} - \sum_{k=1}^{d} \xi_k \frac{a_{i+\frac{1}{2}e_{k}} - a_{i-\frac{1}{2}e_{k}}}{h}  \\
	= \sum_{k=1}^{d} \frac{- a_{i+\frac{1}{2}e_{k}} u_{i+e_k} + [a_{i+\frac{1}{2}e_{k}} + a_{i-\frac{1}{2}e_{k}}] u_i - a_{i-\frac{1}{2}e_{k}} u_{i-e_k}}{h^2} - \sum_{k=1}^{d} \xi_k \frac{a_{i+\frac{1}{2}e_{k}} - a_{i-\frac{1}{2}e_{k}}}{h} = 0.
\end{multline*}
Which can be written as  $(L_a U)_i = (b_a)_i$ where 
\begin{align}
	\label{def:L_a}
	(L_a u)_i &:= \sum_{k=1}^{d} \frac{-a_{i+\frac{1}{2}e_k} u_{i+e_k} + (a_{i-\frac{1}{2}e_k} + a_{i+\frac{1}{2}e_k})u_i - a_{i-\frac{1}{2}e_k} u_{i-e_k} }{h^2}\\
	\label{def:b_a}
	(b_a)_i &:= \sum_{k=1}^{d} \frac{\xi_k (a_{i+\frac{1}{2}e_k} - a_{i-\frac{1}{2}e_k})}{h} .
\end{align}
Next, to summarize later in matrix form, consider $d=1$ (for $d>1$ the process is the same, only the algorithm in computing coefficients matrix need more explanations which addressed in NLSE section by details). Begin by defining matrices, and set the boundary condition to Dirichlet boundary condition $u_{\vert_{\partial\Omega}} = 0$, 
\begin{equation*}
	L_{a} = \frac{1}{h^2}\begin{bmatrix}
		a_{1 - \frac{1}{2}e_{1}} + a_{1 + \frac{1}{2}e_{1}}  &  -a_{1+\frac{1}{2}e_{1}} &  &  &  \\
		-a_{2-\frac{1}{2}e_{1}}&a_{2 - \frac{1}{2}e_{1}} + a_{2 + \frac{1}{2}e_{1}}& -a_{2 + \frac{1}{2}e_{1}} & & \\
		& \ddots& \ddots &\ddots  &  \\
		& & -a_{n-1-\frac{1}{2}e_{1}}&a_{n-1 - \frac{1}{2}e_{1}} + a_{n-1 + \frac{1}{2}e_{1}}&-a_{n-1 + \frac{1}{2}e_{1}}\\
		& & & -a_{n-\frac{1}{2}e_{1}} &a_{n-\frac{1}{2}e_{1}} + a_{n+\frac{1}{2}e_{1}}
	\end{bmatrix}_{n \times n},
\end{equation*}
\begin{equation*}
	b_a = \frac{1}{h}\begin{bmatrix}
		\xi_1 (a_{1+\frac{1}{2}e_{1}} - a_{1-\frac{1}{2}e_{1}}) \\
		\xi_1 (a_{2+\frac{1}{2}e_{1}} - a_{2-\frac{1}{2}e_{1}}) \\
		\vdots \\
		\xi_1 (a_{n+\frac{1}{2}e_{1}} - a_{n-\frac{1}{2}e_{1}})
	\end{bmatrix}_{n \times 1},
	U = \begin{bmatrix}
		u_{1} \\
		u_{2} \\
		\vdots \\
		u_{n}
	\end{bmatrix}_{n \times 1}.
\end{equation*}
Then we can write $L_a U = b_a$. However, our boundary condition is not Dirichlet. To apply periodic boundary condition, one needs to patch some elements to $L_a$ and $U$ as follows
\begin{equation*}
	L_{a}^{\text{patched}} = \frac{1}{h^2}\begin{bmatrix}
		a_{1 - \frac{1}{2}e_{1}} + a_{1 + \frac{1}{2}e_{1}}  &  -a_{1+\frac{1}{2}e_{1}} &  &  & & &-a_{1-\frac{1}{2}e_{1}}\\
		&& \ddots& \ddots &\ddots  &&  \\
		-a_{n+\frac{1}{2}e_{1}}&& & & -a_{n-\frac{1}{2}e_{1}} &a_{n-\frac{1}{2}e_{1}} + a_{n+\frac{1}{2}e_{1}}
	\end{bmatrix}_{n \times n},
\end{equation*}
\begin{equation*}
	U = \begin{bmatrix}
		u_{1} \\
		u_{2} \\
		\vdots \\
		u_{n}
	\end{bmatrix}_{n \times 1},
\end{equation*}
with $a_{1-\frac{1}{2}e_{1}} = \frac{a_{n} + a_{1}}{2}$ and $a_{n+\frac{1}{2}e_{1}} = \frac{a_{1} + a_{n}}{2}$.
The iterative schema needed to generate the data set with sufficient stopping condition is as follow : 
\begin{algorithm}[H]
	\caption{Calculate $U_a$}
	\label{eq:effective_conductance_FDM_Matrix_form}
	\begin{algorithmic} 
		\State $U^0 \leftarrow 0$;
		\While{(Stop condition is not satisfied)}
		\State Solve $L_a^{\text{patched}} U = b_a$ for $U$ with $U^0 \leftarrow U^i$;
		\State $U^{i+1} \leftarrow U^{i}$;
		\EndWhile
		\State $U_a \leftarrow U^{i}$;
	\end{algorithmic}
\end{algorithm}
Plugging the $u_a$ obtained from algorithm (\ref{eq:effective_conductance_FDM_Matrix_form}) to
\begin{equation}
	\label{_A_eff_matriceForm}
	A_{\text{eff}}(a) = h^d (u_{a}^{T} L_{a}^{\text{patched}} u_{a} - 2u_{a}^{T}b_a + a^T 1) 
\end{equation}
will give us $\mathscr{A}_{\text{eff}}$. To generalize this idea to higher dimensions, one can write $L_a^{\text{patched}}$ and $U$ for each dimension, then concatenating $L_a^{\text{patched}}$ horizontally and $U$ vertically gives required matrices to proceed with the rest of the algorithm. Note that in this case, for $b_a$ we will compute the sum of $b_a\text{'s}$ in each dimension. In 1D, since we have the analytic solution for effective conductance as defined in (\ref{ECIM_AnalythicalSolution}), then we can write $\mathscr{A}_{ex} - \mathscr{A}_{app} < \epsilon$ where $\mathscr{A}_{app}$ is the approximation which can be obtained via substituting current $U^{i+1}$ in (\ref{_A_eff_matriceForm}). 
\subsection{Theoretical Justification of Deep Neural Network Representation}
Tough everything we said till now together would show that we can solve PDEs with neural networks, here, our primary goal is not that. Instead, we will prove that we can represent a map from coefficient field to our quantities of interest via convolutional neural networks.\\
The main idea is to view the solution $u$ of the PDE as being obtained via time evolution, where each layer of the NN corresponds to the solution at discrete time step. In other words, mapping the input a from the first layer to last layer in the NN resembles the time-evolution of a PDE with discrete time-steps. We focus here on the case of solving elliptic equations with inhomogeneous coefficients. Similar line of reasoning can be used to demonstrate the representability of the ground state-energy $E_0$ as a function of a using an NN.
\begin{assumption}
	Let 
	\begin{equation}
		\label{def:Epsilon}
		\mathcal{E}(u;a) := \frac{h^d}{2} (u^T L_a u - 2u^T b_a + a^T 1).
	\end{equation} 
	One can see immediately that $\mathscr{A}_{\eff} = 2 \min_{u\in \mathbb{R}^{n^d}} \mathcal{E}(u;a)$.
\end{assumption} 
Due to the variational characterization of (\ref{def:Epsilon}), we can minimize $\mathcal{E}(u;a)$ over the solution space, using steepest decent:
\begin{equation}
	\label{steepest_decent}
	u^{m+1} = u^m - \Delta t \frac{\partial \mathcal{E}(u;a)}{\partial u} = u^m - \Delta t( L_a u^m - b_a),
\end{equation}
where $\Delta t$ is a step size (called learning rate in machine learning context) chosen sufficiently small to ensure decent of the energy. The optimization problem is convex due to the ellipticity assumption of the coefficient field a in (\ref{def:coefficient_field_for_effective_conductance}) with Lipshitz continuous gradient (which ensures $u^T L_a u > 0$ except for $u=1$). Therefore the iterative schema converges to the minimizer with proper choice of the step size for any initial condition. Thus we can choose $u^0 = 0$.\\
At each step, we need to approximate the map from $u^m$ to $u^{m+1}$ in (\ref{steepest_decent}) using NN approximation. Hence, the process of time-evolution is similar to applying noisy gradient decent on $\mathcal{E}(u;a)$. More precisely, after performing a step of gradient decent update, the NN approximation incur noise to the update, i.e.
\begin{equation}
	\label{eq:iterationschema_noisy_steepest_decent}
	v^0 = u^0 = 0,\hspace*{.5cm} u^{m+1} = v^m - \Delta t \nabla \mathcal{E}(v^m;a),\hspace*{.5cm} v^{m+1} = u^{m+1} + \Delta t \gamma^{m+1}.
\end{equation}
Here $\gamma^{m+1}$ is the error for each layer of the NN in approximating each exact time-evolution iteration $u^{m+1}$. To be sure, instead of $u^{m}$, the object that is evolving in the NN as $m$ changes is $v^m$.
\begin{assumption}
	We assume $a\in \mathscr{A} = {a\in \mathbb{R}^n \vert a_i \in [\lambda_{0}, \lambda_{1}], \forall i }$ with $\lambda_{0} > 0$. Under this assumption $\lambda_{a} := \norm{L_a}_{2}$ and $\mu_a := \frac{1}{\norm{L_{a}^{\dag}}_2}$ satisfy
	\begin{equation}
		\label{ass:lambda_a_properties}
		\lambda_{a} = \mathcal{O}(\lambda_{1} h^{d-2}), \hspace*{.5cm} \mu_a = \Omega(\lambda_{0} h^d).
	\end{equation}
	Here for matrices $\norm{\cdot}_{2}$ denotes the spectral norm, $h=1/n$. 
\end{assumption}
\begin{assumption}
	We assume the NN results an approximation error term $\gamma^{m+1}$ with properties
	\begin{eqnarray}
		\label{ass:ass2}
		\norm{\gamma^{m+1}}_{2} \leq c \norm{\nabla \mathcal{E}(v^m;a)}_{2}, \\
		\textbf{1}^T \gamma^{m+1} = 0, 
	\end{eqnarray}
	for $m=0,\dots ,M-1$ when approximating each step of time-evolution.
\end{assumption}
\begin{lemma}
	\label{lemma1}
	The iteration in (\ref{eq:iterationschema_noisy_steepest_decent}) satisfies
	\begin{equation}
		\mathcal{E}(v^{m+1};a) - \mathcal{E}(v^m;a) \leq - \frac{\Delta t}{2} \norm{\nabla \mathcal{E}(v^m;a)}_{2}^{2},
	\end{equation}
	and $\Delta t \leq \delta$, $\delta = (1-\frac{1}{2(1-c)}) \frac{2}{\lambda_{a}'}$ with $\lambda_{a}' = (1+ \frac{c^2}{1-c}) \lambda_{a}$. Furthermore, 
	\begin{equation}
		\label{eq:toproof_in_lemma1}
		\frac{\Delta t}{2} \sum_{m = 0}^{M-1} \norm{\mathcal{E}(v^{m+1};a)}_{2}^{2} \leq \mathcal{E}(v^0;a) - \mathcal{E}(v^M;a) \leq \mathcal{E}(v^0;a) - \mathcal{E}(u^*).
	\end{equation}
	\begin{proof}
		From Lipshitz property of $\nabla \mathcal{E}(u;a)$,
		\begin{align*}
			\mathcal{E}(v^{m+1};a) - \mathcal{E}(v^m; a) & \leq \innerProduct{\nabla \mathcal{E}(v^m;a)}{v^{m+1} - v^m} + \frac{\lambda_{a}}{2} \norm{v^{m+1} - v^m}_{2}^{2}\\
			&= \innerProduct{\nabla \mathcal{E}(v^m;a)}{ v^m - \Delta t(\nabla \mathcal{E}(v^m;a) + \gamma^{m+1}) - v^m} \\
			&+ \frac{\lambda_{a}}{2} \norm{v^m - \Delta t (\nabla \mathcal{E}(v^m;a) + \gamma^{m+1})}_{2}^{2} \\
			&= - \Delta t(1-\frac{\Delta t \lambda_{a}}{2}) \norm{\nabla \mathcal{E}(v^m;a)}_{2}^{2}\\
			&+ \Delta t (1- \frac{\Delta t \lambda_{a}}{2}) \innerProduct{\gamma^{m+1}}{\nabla \mathcal{E}(v^m;a)} + \frac{\lambda_{a} \Delta t^2}{2} \norm{\gamma^m}_{2}^{2} \\ 
			&\leq -\Delta t (1-\frac{\Delta t \lambda_{a}}{2}) \norm{\nabla \mathcal{E}(v^m;a)}_{2}^{2} \\
			&+ c\Delta t (1- \frac{\Delta t \lambda_{a}}{2} + \frac{c\Delta t \lambda_{a}}{2}) \norm{\nabla \mathcal{E}(v^m;a)}_{2}^{2}\\
			&= - \Delta t \big( (1-c) - (1-c+c^2) \frac{\Delta t \lambda_{a}}{2} \big) \norm{\nabla \mathcal{E}(v^m;a)}_{2}^{2}\\
			&=-\Delta t (1-c) \big( 1- \frac{1-c+c^2}{1-c} \frac{\Delta t \lambda_{a}}{2} \big) \norm{\nabla \mathcal{E}(v^m;a)}_{2}^{2} \\
			&=-\Delta t (1-c) (1- \frac{\Delta t \lambda_{a}'}{2}) \norm{\nabla \mathcal{E}(v^m;a)}_{2}^{2}.
		\end{align*}
		Letting $\Delta t \leq \big(1-\frac{1}{2(1-c)}\big) \frac{2}{\lambda_{a}'}$, we get 
		\begin{equation}
			\mathcal{E}(v^{m+1};a) - \mathcal{E}(v^m;a) \leq -\frac{\Delta t}{2} \norm{\nabla \mathcal{E}(v^m;a)}_{2}^{2}
		\end{equation}
		Summing the left hand side and the right hand side gives (\ref{eq:toproof_in_lemma1}). this concludes the lemma.
	\end{proof}
\end{lemma}
\begin{thm}
	\label{theorem:one}
	If $\Delta t$ satisfies the the condition in Lemma \ref{lemma1}, given any $\epsilon > 0$, $\vert \mathcal{E}(v^M;a) - \mathcal{E}(v;a)\vert \leq \epsilon$ for $M = \mathcal{O}((\frac{\lambda_{1}^2}{\lambda_{0}} + \lambda_{1}) \frac{n^2}{\epsilon})$.
	\begin{proof}
		Since by convexity 
		\begin{equation}
			\mathcal{E}(u^*;a) - \mathcal{E}(v^m;a) \geq \innerProduct{\nabla \mathcal{E}(v^m;a)}{u^* - v^m},
		\end{equation}
		along with Lemma \ref{lemma1},
		\begin{align*}
			\mathcal{E}(u^*;a) &\leq \mathcal{E}(u^*;a) + \innerProduct{\nabla \mathcal{E}(v^m;a)}{v^m - u^*} - \frac{\Delta t }{2} \norm{\nabla \mathcal{E}(v^m;a)}_{2}^{2}\\
			&= \mathcal{E}(u^*) + \frac{1}{2\Delta t} (2\Delta t \innerProduct{\nabla \mathcal{E}(v^m;a)}{ v^m - u^*} - \Delta t^2 \norm{\nabla \mathcal{E}(v^m;a)}_{2}^{2} \\
			&+ \norm{v^m - u^*}_{2}^{2} - \norm{v^m - u^*}_{2}^{2})\\
			&= \mathcal{E}(u^*;a) + \frac{1}{2 \Delta t}(\norm{v^m-u^*}_{2}^{2} - \norm{v^m - \Delta t \nabla \mathcal{E}(v^m;a) - u^*}_{2}^{2})\\
			&= \mathcal{E}(u^*;a) + \frac{1}{2 \Delta t}(\norm{v^m-u^*}_{2}^{2} - \norm{v^m - \Delta t \gamma^{m+1} - u^*}_{2}^{2})\\
			&= \mathcal{E}(u^*;a) + \frac{1}{2 \Delta t}(\norm{v^m-u^*}_{2}^{2} - \norm{v^{m+1} - u^*}_{2}^{2}\\
			&+ 2\Delta t \innerProduct{\gamma^{m+1}}{v^{m+1}-u^*} - \Delta t^2 \norm{\gamma^{m+1}}_{2}^{2})\\
			&= \mathcal{E}(u^*;a) + \frac{1}{2 \Delta t}(\norm{v^m-u^*}_{2}^{2} - \norm{v^{m+1} - u^*}_{2}^{2} + \Delta t^2 \norm{\gamma^{m+1}}_{2}^{2}\\
			&+ 2\Delta t \innerProduct{\gamma^{m+1}}{v^{m}-u^*} - 2\Delta t \innerProduct{\gamma^{m+1}}{\nabla \mathcal{E}(v^m;a)})\\
			&\leq \mathcal{E}(u^*;a) + \frac{1}{2 \Delta t}(\norm{v^m-u^*}_{2}^{2} - \norm{v^{m+1} - u^*}_{2}^{2} + \Delta t^2 \norm{\gamma^{m+1}}_{2}^{2}\\
			&+ 2\Delta t \norm{\gamma^{m+1}}_{2} (\norm{v^m - u^*}_{2}^{2} + \norm{\nabla \mathcal{E}(v^m;a)}_{2}))\\
			&\leq \mathcal{E}(u^*;a) + \frac{1}{2 \Delta t}(\norm{v^m-u^*}_{2}^{2} - \norm{v^{m+1} - u^*}_{2}^{2} + \Delta t^2 \norm{\gamma^{m+1}}_{2}^{2}\\
			&+ 2\Delta t (1+ \frac{2}{\mu_a}) \norm{\gamma^{m+1}}_{2} \norm{\nabla \mathcal{E}(v^m;a)}_{2})\\
			&\leq \mathcal{E}(u^*;a) + \frac{1}{2 \Delta t}(\norm{v^m-u^*}_{2}^{2} - \norm{v^{m+1} - u^*}_{2}^{2} + c^2 \Delta t^2 \norm{\nabla \mathcal{E}(v^m;a)}_{2}^{2}\\
			&+ 2c (1+ \frac{2}{\mu_a}) \Delta t \norm{\nabla \mathcal{E}(v^m;a)}_{2}^{2}).
		\end{align*}
		The last inequality follows from (\ref{ass:lambda_a_properties}), which implies $\norm{L_a u}_{2} \geq \mu_a \norm{u}_{2}$ if $u^T\textbf{1}=0$. More precisely, the fact that $v^0 = 0$, $\nabla\mathcal{E}(u)^T \mathbf{1} = 0$ (follows from the form of $L_a$ and $b_a$ defined in (\ref{def:L_a}, \ref{def:b_a})), and $(\gamma^m)^T \textbf{1} = 0$ $\forall m$(due to the assumption in (\ref{ass:ass2})) implies $(v^m)^T \textbf{1} = 0$, hence $\frac{\mu_a}{2} \norm{v^m - u^*}_{2} \leq \norm{\triangle \mathcal{E}(v^m;a) - \triangle \mathcal{E}(u^*;a)}_{2} = \norm{\triangle \mathcal{E}(v^m;a)}_{2}$. Reorganizing our last inequality, we get
		\begin{multline}
			\mathcal{E}(v^m;a) - \mathcal{E}(u^*;a) \\
			\leq \frac{1}{2\Delta t} \bigg(\norm{v^m - u^*}_{2}^{2} - \norm{v^{m+1} - u^*}_{2}^{2} +c\Delta t (c\Delta t + 2(1+\frac{2}{\mu_a})) \norm{\nabla \mathcal{E}(v^m;a)}_{2}^{2} \bigg)
		\end{multline}
		Summing both left and right hand sides results in 
		\begin{multline}
			\mathcal{E}(v^{M};a) - \mathcal{E}(u^*;a) \leq \frac{1}{M} \sum_{m = 0}^{M-1} \mathcal{E}(v^{m+1};a) - \mathcal{E}(u^*;a) \\
			\leq \frac{1}{M} \bigg[ \bigg( \frac{\norm{v^0 - u^*}_{2}^{2}}{2\Delta t}\bigg) + \frac{2c}{\Delta t} \bigg(c\Delta t + 2(1+\frac{2}{\mu_a})\bigg) (\mathcal{E}(v^0;a) - \mathcal{E}(u^*;a)) \bigg]
		\end{multline}
		where the second inequality follows from (\ref{eq:toproof_in_lemma1}). In order to derive a bound for $\norm{v^0 - u^*}_{2}^{2}$, we appeal to strong convexity property of $\mathcal{E}(u;a)$:
		\begin{equation*}
			\mathcal{E}(v^0;a) - \mathcal{E}(u^*;a) \geq \innerProduct{\nabla \mathcal{E}(u^*;a)}{v^0 - u^*} + \frac{\mu_a}{2} \norm{v^0 - u^*}_{2}^{2} = \frac{\mu_a}{2} \norm{v^0 - u^*}_{2}^{2}
		\end{equation*}
		for $\innerProduct{\textbf{1}}{v^0 - u^*} = 0$. The last equality follows from the optimality of $u^*$. Then
		\begin{equation}
			\mathcal{E}(v^{M};a) - \mathcal{E}(u^*;a) \leq \frac{1}{M} \bigg[ \bigg(\frac{1}{\mu_a \Delta t}\bigg)+ \frac{2c}{\Delta t} \bigg(c\Delta t + 2(1+\frac{2}{\mu_a})\bigg)\bigg] (\mathcal{E}(v^0;a) - \mathcal{E}(u^*;a)).
		\end{equation}
		Since $\mathcal{E}(v^0;a) = h^d \frac{a^T \textbf{1}}{2} = \mathcal{O}(\lambda_{1})$, along with $\lambda_{a} = \mathcal{O}(\lambda_{1} h^(d-2))$ and $\mu_a = \Omega(\lambda_{0} h^d)$, we establish the claim.
	\end{proof}
\end{thm}
Theorem (\ref{theorem:one}) will immediately results the following theorem:
\begin{thm}
	Fix an error tolerance $\epsilon > 0$, there exists a neural-network $h_\theta(\cdot)$
	with $\mathcal{O}(n^d)$ hidden nodes per-layer and $\mathcal{O}((\frac{\lambda_{1}}{\lambda_{0}} + 1) \frac{n^2}{\epsilon})$ layers such that for any
	$a \in \mathscr{A} = {a \in \mathbb{R}^n \vert a_i \in [\lambda_{0} , \lambda_{1}], \forall i}$, we have
	\begin{equation*}
		\abs{h_\theta(a) − \mathscr{A}_{\text{eff}}(a)} \leq \epsilon \lambda_{1}.
	\end{equation*}
	Note that due to the ellipticity assumption $a \in \mathscr{A}$, the effective conductivity
	is bounded from below by $\mathscr{A}_{\text{eff}}(a) \geq \lambda_{0} \geq 0$. Therefore the theorem immediately
	implies a relative error bound
	\begin{equation*}
		\frac{\abs{h_\theta(a) - \mathscr{A}_{\text{eff}}(a)}}{\mathscr{A}_{\text{eff}}(a)} \leq \epsilon \frac{\lambda_{1}}{\lambda_{0}}.
	\end{equation*}
\end{thm}
\section{Nonlinear Schr\"{o}dinger Equation with Inhomogeneous Background Potential}
Consider following differential equation:
\begin{equation}
	-\Delta u(x) + a(x)u(x) + \sigma u(x)^3 = E_0 u(x), x\in [0,1]^d, s.t. \int_{[0,1]^d}u(x)^2 dx = 1.
	\label{eq:NLSE}
\end{equation}
In this eigen value problem, given background potential $a(x)$, we are interested to find the smallest eigen value, \textit{the ground state energy}, $E_0$. Here,  $-\Delta u(x)$ represents the kinetic energy and $\sigma u(x)$ represents interaction between particles of the same kind. Let $\sigma = 2$ and thus, consider a defocusing cubic Schr\"{o}dinger equation which can be understood as a model for solition in nonlinear photonics or Bose-Einstein condensate with inhomogeneous media. Similar to the previous section, we would like to solve the discretized version of equation \eqref{eq:NLSE} as follow
\begin{equation}
	(Lu)_i + a_i u_i + \sigma u_{i}^3 = E_0 u_i, \sum_{i=1}^{n^d} u_{i}^2 h^d = 1
\end{equation}
where
\begin{equation*}
	(lu)_i := \sum_{k=1}^{d} \frac{-u_{i+e_{k}} + 2u_i - u_{i-e_{k}}}{h^2}.
\end{equation*}
To solve this equation, we use newton homotopy method as follow. Consider the sequence of NLSE, $\{ (Lu)_i + a_i u_i + s u_{i}^{3} = E_0 u_i \}_{i \in \mathbb{I}}$, with the normalization constraint on u, $h^d \innerProduct{u_i}{u_i} = 1$ with $s = s_1\dots s_k$ where $0<s_1<s_2<\dots<s_k=\sigma$. Consider 2D case. When $s = 0$, the coefficient matrix can be calculated as follows:
\begin{algorithm}[H]
	\caption{Coefficient matrix} 
	\label{alg:NLSE_FDM_Matrix}
	\begin{algorithmic}[1]
		\State $i,j \leftarrow 1$;
		\State Devide $x_1$ to $p$ part and $x_2$ to $q$ part; 
		\For{$r\in\{1, ..., p*q\}$}
		\If{$mod(r,q)==0$}
		\State $j = q$;
		\Else 
		\State $j = mod(r,q)$;
		\EndIf
		\State $i = floor((r-1)/p)+1$;		
		\If{$i>p$}
		\State $i = mod(i,p)$;
		\EndIf
		\If{$j>q$}
		\State $j = mod(j,q)$;
		\EndIf
		\algstore{bkbreak}
	\end{algorithmic}
\end{algorithm} 
\begin{algorithm}[H]
	\begin{algorithmic}[1]
		\algrestore{bkbreak}		
		\If{$i == p$}
		\State $A(r, j) = \frac{1}{h^2}$;
		\Else 
		\State $A(r, ip+j) = \frac{1}{h^2}$;
		\EndIf		
		\If{$i == 1$}
		\State $A(r, (p-1)p+j) = \frac{1}{h^2}$;
		\Else 
		\State $A(r, (i-2)p+j) = \frac{1}{h^2}$;
		\EndIf	
		\If{$j == q$}
		\State $A(r, (i-1)p +1) = \frac{1}{h^2}$; 
		\Else 
		\State $A(r, (i-1)p+(j+1)) = \frac{1}{h^2}$;
		\EndIf	
		\If{$j == 1$}	
		\State $A(r, (i-1)p+q) = \frac{1}{h^2}$;
		\Else 
		\State $A(r, (i-1)p+(j-1)) = \frac{1}{h^2}$;
		\EndIf		
		\State $A(r,r) = - a_{i,j} - \frac{4}{h^2}$;		
		\EndFor		
	\end{algorithmic}
\end{algorithm}
From algorithm \eqref{alg:NLSE_FDM_Matrix}, one can find starting values $E_0^0$ and $U^0$. Note that, for $s = 0$, it is an standard eigenvalue problem; $AU = E_0U$. Also,for $s_i > 0$, there is an additional update operation on the diagonal elements, that is, $A(r,r) = A(r,r) + s_i U_{i,j}^2$. Note that the normalization condition had been applied in the method implicitly. Hence, what the algorithm computes is $E\times h^2$ so there remains another operation $E = E/h^2$. For each $S_i$, $i>0$, inverse power method used to solve the NLSE and $E_0$, $V_0$ obtained with $s=s_i$ will be used to warm start for the next iteration. The procedure will proceed till $s_k = \sigma$. We used step size equals to $0.4$ for that purpose.\\
In the rest of this thesis, we use algorithm~\eqref{eq:effective_conductance_FDM_Matrix_form} and equation~\eqref{eq:NLSE} as our numerical schema for generating the Data set needed for training and testing the Neural Network that will be discussed in details in next chapter.
