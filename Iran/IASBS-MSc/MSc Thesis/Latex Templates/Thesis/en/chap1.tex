\chapter{Introduction}

\pagenumbering{arabic}
\setcounter{page}{1}
This chapter begins by introducing the basic concepts required for the rest of the thesis, such as uncertainty quantification and neural networks. It follows by introducing two partial differential equations that we want to solve. Furthermore, a brief report of the results is given. This chapter is concluded by an overview of the structure of the thesis. 

\section{Background \& Motivation}
\label{sec:Background_And_Motivations}
To model real-world phenomenons, one makes assumptions to simplify his model. More often than not, the hidden parameters that take part in the model are unknown. In many other cases, the exact value of the inputs is unclear. These are examples of what called uncertainty. Usually, one wants to quantify these uncertainties to control them, if possible.\\ Often, models are consist of a single or a system of Partial Differential Equations (PDEs). Then it is beneficial to study methods to solve PDEs that contain uncertainties in their structures. However, incorporating uncertainty would increase both complexity and computational time.\\
Our interests come from modeling the mechanical behavior of soft tissue, breast in particular, under compression. Due to its importance in diagnosing and treatment of breast cancer, it is an exciting subject for many \cite{khatam2015vivo, tanner2002comparison, han2011development, azar2002methods, azar2001deformable, del2008finite, martinez2017finite}. Ultimately, we are after answering this question: `How can we make it both patient-specific and efficient to use in actual clinics?'. 

The first challenge is that different bodies have different properties, e.g., elasticity. To our knowledge, there is no definite way to determine the exact value of these properties yet. Then they can be thought of as uncertain coefficients in the model. The second challenge is that the results of such modeling must be computable in clinical time. 

On the other hand, it is promising to use machine learning for numerical approximations \cite{geneva2020modeling, smaoui2004modelling, lagaris2000neural, beck2019machine, zhang2019quantifying, parisi2003solving}. As an example, for being saved from the curse of dimensionality, one may compute the solution of a PDE on a mesh, then feed that answer to a neural network and use it to find the solution on a finer mesh. Another may find a surrogate forward model which describes a map between the coefficient field to the quantities of interests rather than solving the PDE itself. 

To divide the problem into subproblems, we decide to consider solving PDEs with uncertainties using neural networks. In this thesis, we will study the use of such a method on two PDEs to cover both linear and nonlinear cases.

\section{Partial Differential Equations}
\label{sec:PDE}
The idea behind the partial differential equation is simple; to describe the variation of a physical quantity that depends on multiple physical variables, one has to find its partial variation with respect to each physical variable while keeping others constant. Then, summing up all the variations shall give the overall variation of that quantity. Then, a partial differential equation is a mathematical equation that involves two or more independent variables, an unknown function (which depends on those variables), and partial derivatives of the unknown function with respect to the independent variables. The order of a partial differential equation is the order of the highest derivative involved. A solution (or a particular solution) to a partial differential equation is a function that solves the equation or, in other words, turns it into an identity when substituted into the equation. A solution is called general if it contains all particular solutions of the equation concerned \cite{stavroulakis1999partial}.\\
PDEs have been used  to formulate the solution of physical problems involving functions of several variables mathematically such as the propagation of heat or sound, fluid flow, elasticity, electrostatics, electrodynamics, etc \cite{stavroulakis1999partial}.

If all the terms of a PDE contain the dependent variable or its partial derivatives, then such a PDE is called inhomogeneous partial differential equation or homogeneous otherwise. The external forces to the PDE models have applied through the inhomogeneous term.

One needs additional information about the initial state of the model or change of it over the boundary called conditions to find the unique solution of a PDE. 
There are two types of conditions, initial conditions, and boundary conditions. An initial condition expresses the value of the solution or its derivatives at an initial point in time. A boundary condition, on the other hand, defines the value of the solution or its derivatives at the boundary of the domain of the problem.\\
Boundary conditions can be categories into three categories; \textit{Dirichlet} boundary condition defines the value of the exact solution.  \textit{Neumann} boundary condition describes the values of the derivatives of the exact solution. Finally, \textit{Robin} boundary condition describes both the solution and the derivatives. Note that the Robin boundary condition can formulate as a linear combination of the Dirichlet and Neumann boundary conditions. 
\section{Uncertainty Quantification}
As mentioned in Section \eqref{sec:Background_And_Motivations}, one aspect of this work is Uncertainty Quantification (UQ). This section gives an overview of what is 'uncertainty quantification' and how can it be applied to models.

``UQ is the end-to-end study of the reliability of scientific inference... one is interested in relationships between pieces of information, not the `truth' of those information/assumptions... UQ cannot tell you that your model is `right' or `true', but only that, if you accept the validity of the model (to some quantified degree), then you must logically accept the validity of certain conclusions (to some quantified degree) ''\cite{UQIntro_Sullivan}.\\
There are two types of uncertainty; uncertainty about an inherently variable phenomenon known as `Aleatoric' and uncertainty emerging from the lack of knowledge known as `Epistemic'. Epistemic uncertainty concerns the correctness of the structure of the model itself. Aleatoric uncertainty, on the other hand, concerns the correct values of the parameters of the model.
In a broad view, a common UQ task is seeking one or more of these objectives 
\begin{enumerate}[i.]
	\item The \textit{forward propagation} or \textit{push-forward} problem; 
	\item The \textit{reliability} or \textit{certification} problem;
	\item The \textit{prediction} problem;
	\item The \textit{inverse} problem;
	\item The \textit{model reduction} or \textit{model calibration} problem; 
\end{enumerate}

UQ in applications often involves the study of PDEs with the random coefficient field. To understand the behavior of a system in the presence of uncertainties, one can extract PDE-derived physical quantities as functionals of their coefficient fields. Even with a suitable discretization of the PDE domain, and the range of the random variables, this can potentially need solving PDE an exponential number of times numerically. Fortunately, in most PDE applications these functionals often depend only on a few characteristic ``features'' of the coefficient fields, enabling them to be determined from solving PDE a limited number of times.\\

\section{Artificial Neural Network}
Inspired by how the brain works, in 1943, Warren McCulloch and Walter Pitts \cite{mcculloch1943logical} wrote a paper on how neurons might work. They modeled a simple neural network with electrical circuits. Although the study of the human brain is thousands of years old, this was the beginning of a new field that evolve into what we know today as Artificial Neural Networks (ANN).\\
The short history of the development of deep neural networks goes as follow:\\
``In the late 1940s, D. O. Hebb created a learning hypothesis that became known as Hebbian learning \cite{hebb1949organization}. 
The first functional networks with many layers called `Group Method of Data Handling' were created by Ivakhnenko and Lapa in 1965 \cite{schmidhuber2015deep, ivakhnenko1973cybernetic, ivakhnenko1967cybernetics}. 
The basics of continuous backpropagation \cite{schmidhuber2015deep, dreyfus1990artificial, mizutani2000derivation} were derived in the context of control theory by Kelley \cite{kelley1960gradient} in 1960 and by Bryson in 1961, using principles of dynamic programming \cite{bryson1961gradient}.\\
In 1970, Seppo Linnainmaa published the general method for Automatic Differentiation (AD) of discrete connected networks of nested differentiable functions \cite{linnainmaa1970representation, linnainmaa1976taylor}.
Geoffrey Hinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine \cite{smolensky1986information} to model each layer. In 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images \cite{le2013building}. Unsupervised pre-training and increased computing power from Graphics Processing Units (GPU) and distributed computing allowed the use of larger networks, particularly in the image and visual recognition problems, which became known as "deep learning".\\
Neural Networks, as we know them today, are network or circuit of real biological neurons or their artificial counterparts. In the latter case, we call it artificial neural network. In this thesis, we are working strictly on ANN and we might use the term Neural Network (NN) for simplicity.\\ 
ANNs are composed of artificial neurons that hold the biological concept of neurons that are receiving input, combine it with their internal state and turn on or off. They do that using weights, bias and activation function.\\
The weights have used to model the connections of the biological neurons. A \textbf{weight} is ``a parameter associated with a connection from one neuron $M$, to another neuron $N$. It corresponds to a synapse in a biological neuron, and it determines how much notice the neuron $N$ pays to the activation it receives from neuron $M$. If the weight is positive, the connection is called excitatory, while if the weight is negative, the connection is called inhibitory'' \cite{MLDict}.\\
``In feedforward and some other neural networks, each hidden unit, and each output unit is connected via a trainable weight to a unit (the \textbf{bias} unit) that always has an activation level of â€“1. This gives a trainable threshold equal to the value of the weight from the bias unit to each hidden or output unit'' \cite{MLDict}.
Moreover, \textbf{activation function} ``is the function that describes the output behavior of a neuron. Most network architecture starts by computing the weighted sum of the inputs (that is, the sum of the product of each input with the weight associated with that input). This quantity, the total net input, is then usually transformed in some way, using what is sometimes called a squashing function. The simplest squashing function is a step function: if the total net input is less than 0 (or more generally, less than some threshold T) then the output of the neuron is 0, otherwise, it is 1. A common squashing function is a logistic function. In summary, the activation function is the result of applying a squashing function to the total net input'' \cite{MLDict}. The important characteristic of the activation function is that it provides a smooth, differentiable transition as input values change, i.e. small changes in input would produce small changes in output.

Mathematically, we can model a neuron as follow:
\begin{equation}
\psi(XW+b).
\end{equation} 
Here, \textit{X} is the input vector, \textit{W} is the weight matrix, \textit{b} is the bias and finally, $\psi$ is the activation function. With this notation, the final output of our feedforward network would be:
\begin{equation}
\Phi(X,W^1,\dots,W^K) = \psi_{k}(\psi_{k-1}(\dots \psi_{2}(\psi_{1}(XW^1+b^1)W^2+b^2)\dots W^{K-1}+b^{K-1})W^{K}+b^{K}).
\end{equation}
The initial inputs of the ANN are external data, such as images and documents. The ultimate output is to accomplish the task, such as recognizing an object within the image.\\
Two general types of learning exist; learning from the data associated with labels which are called supervised learning and learning from unlabeled data which is called unsupervised learning. In this thesis, we would do supervised learning. \\
In general, the neural network would go through a process that we call learning. Learning, in case of supervised learning is to minimize a function like $ \mathscr{L}(Y, \Phi(X, W^1,\dots, W^K))$ (i.e. \textbf{loss} function e.g. mean square error) that defines a relation between the network output $\Phi(X, W^1,\dots, W^K)$ and the expected network output \textit{Y}, which is the labels associated with the data:
\begin{equation}
\min_{\{W^k\}_{k=1}^{K}} \mathscr{L}(Y, \Phi(X,W^1,\dots,W^K)).
\end{equation}
If the network were unable to minimize the loss function, we say that \textbf{underfitting} happened.\\
To ensure that the network learned correctly, we have to test it against a fresh dataset which it hasn't see before, which called the test dataset. What we expect is an acceptable result from our network over this dataset concerning some metrics (e.g mean absolute error) which if satisfied, we say that the network learned successfully. If not, we would say that \textbf{overfitting} happened, which means that the network was unable to generalize the hidden relation of the training dataset (i.e. the provided data set in learning phase) to the test dataset though it has perfect results on the training dataset itself.\\
%Often the weights of the network would be initialized randomly but as it's explained in Section \eqref{sec:overviewof_FEM_FDM}, we initialized our network with the zero matrices for weights. 
The learning process begins by feeding a \textbf{batch} of train data to the network. Next, the network would try to adjust it's weighted so that it minimizes loss function and after that, the next batch would feed the network. Each passes through all of the train data would be called an \textbf{epoch}. The whole learning process would often consist of many epochs. But one must be careful about overfitting the data with too many epochs.\\
Now, we would like to discuss the questions which this thesis is based upon. `Why one may interested in solving a PDE, especially in the presence of uncertainty, using neural network, and how?' To highlight the important aspect of the idea of using a neural network for solving PDEs, consider these questions. Would it be nice if we can solve our problem on a bigger mesh to train our network and then use it to compute the solution over a finer mesh? How about solving problems with high dimensions in which the classical numerical methods such as finite element method and finite difference method suffer the curse of dimensionality? As we would see in the next section, computing the basis function over nodal points is the source of heavy computational cost in the finite element method. What if we would be able to bypass the need to compute them? These are just some of the applications which one may find when importing neural network as a tool in numerical analysis.\\
There are two different choices to generate data sets when dealing with PDEs. First, solve the PDE over a bigger mesh to obtain the train data set and then solving the PDE over a finer mesh to generate test dataset. Second, picking a percentage of the data points concerning normal distribution over the domain. Moreover, to approach the problem, methods can be categorized into three categories. First, we know the PDE and its coefficients. What we seek is to solve the PDE directly using neural networks. In that regard, one may use the Ritz variational formulation of the PDE to define the lost function and uses the Stochastic Gradient Decent (SGD) optimization algorithm as the optimizer. This way, a numerical approximation of the solution can be calculated much like any other numerical methods \cite{weinan2018deep}, or else, one may use a coarse grid to train the network and then use the trained model on a fine grid mesh \cite{wang2019prediction}. Second, the PDE is known but the coefficients are unknown \cite{raissi2017physics, Base_paper}. Finally, neither PDE nor it's coefficients are known \cite{raissi2018deep}. \\
Consider some PDEs with the random coefficient field, we are interested in finding our physical quantities as functionals which depend only on a few characteristic ``features'' of the coefficient fields through our neural network. A dimension reduction technique based on neural network representation has used to do regression.\\
The fundamental task of regression seeks to find a function $h_{\theta}$ parameterized by a parameter vector $\theta \in \mathbb{R}^{p}$ such that
\begin{equation}
f(\theta) \approx h_{\theta}(a), a\in \mathbb{R}^{q}
\label{regression_key_task}
\end{equation}
However, choosing a sufficiently large class of approximation functions without the issue of over-fitting remains a delicate business. For example when choosing the set of basis $\{\phi_k(a)\}$ such that $f(a) = \sum_{k} \beta_k \phi_k(a)$ in linear regression.\\ 
A key advantage of using neural network is that it bypasses the traditional need to handcraft basis for spanning $f(a)$ as in linear regression. Instead, it directly learns an approximation that satisfies \eqref{regression_key_task} in a data-driven way.
More precisely, we want to learn $f(a)$ that maps the random coefficient vector $a$ in a PDE to some physical quantities described by the PDE.\\
The approach is conceptually simple, consisting of the following steps:
\begin{enumerate}[i.]
	\item Sample the random coefficients (a in \eqref{regression_key_task}) of the PDE from a user-specified distribution. For each set of coefficients, solve the deterministic PDE to obtain the physical quantity of interest ($f(a)$ in \eqref{regression_key_task}).
	\item Use a neural network as the surrogate model $h_{\theta}(a)$ in \eqref{regression_key_task} and train it using the previously obtained samples.
	\item  Validate the surrogate forward model with more samples. 
\end{enumerate}

We note that our work is a report of \cite{Base_paper}. In this thesis, the function that we want to parameterize is over the coefficient field of the PDE. 
It would be beneficial to mention other works which try to solve deterministic PDE numerically using a neural network \cite{weinan2018deep, khoo2019solving, lagaris1998artificial, long2017pde, rudd2015constrained}, and \cite{han2018solving} where a deterministic PDE is solved as a stochastic control problem using neural network. \\

\section{An Overview of Classical Numerical Methods}
\label{sec:overviewof_FEM_FDM}
One of the popular  numerical method for solving PDEs is  Finite Difference Method (FDM). It is based on the idea of substituting the derivatives in the equation with their approximations. To do so, one may define a mesh, write down the value of each partial derivative on each nodal point of the mesh and then calculating a numerical approximation for the solution of the partial differential equation. The precision of the method has a direct relation with the order of the precision which we chose to approximate the derivatives in and it can be computed through the Taylor expansion. Though the FDM is a good method and it is widely used, it has its downsides too. An important one is that the FDM cannot be used on complex domains \cite{forsythe1960finite, smith1985numerical, morton2005numerical}.\\
Finite Element Method (FEM), uses more complex mesh structures and it can be used on more complex domains. It is based on the variational formulation of the equation. There are two kinds of finite element methods: \textbf{Ritz} formulation in which we would write an equivalent minimization problem and we would solve this new problem instead, and \textbf{Galerkin} formulation in which we would use the basics of integration by part and Green's identities to reduce the order of the differential equation and then we try to solve this new problem. Either way, we would write an equivalent problem to the desired differential equation using basis functions. After we achieved the variational formulation, we would approximate its solution by reducing the infinite dimension of the solution space to a finite dimension. Hence, we call the method, \textbf{finite element method} \cite{suli2012lecture, johnson2012numerical}.\\
The finite element method achieved a great deal of success in general and is very popular among mathematicians and engineers, but there is some downside to it as well. We cannot use it for high dimensional problems, which often in literature referred to as \textit{the curse of dimensionality}. Moreover, refinement of the mesh would result in increasing the nodal points count which means an increase in computational cost. Finally, we can explore more domains with the finite element method, yet the domain has to be Lipschitz. We would study finite element method in more details in Section \eqref{sec:finite_element_method} \cite{zienkiewicz1977finite, cook2007concepts}.
%New methods always come to conquer the limitations of their ancestors and improve them.
For the enthusiastic reader, here is a list of some other numerical methods: wavelet method \cite{dahmen1997multiscale, lepik2005numerical}, finite volume method \cite{moukalled2016finite, leveque2002finite} and meshfree method \cite{liu2003smoothed, liu2009meshfree}.\\
\section{Problem Statement and Contributions}
As we saw in Section \eqref{sec:PDE}, PDEs come to life to express the relations between changing quantities. In this section, we discuss a bit about the problems which we want to solve.
Suppose we have an inhomogeneous media (an inhomogeneous media is a medium in which all properties of interest are not the same at any point). Also, suppose that we have different values for conductance for different materials, which composed our media, modeled by $a(x)$. Given the fixed direction vector $\xi \in \mathbb{R}^d$, we are interested to know the effective conductance through the media in that direction. Mathematically, we want to find the solution to the following minimization problem: 
\begin{equation}
A_{\eff}(a) = \min_{u(x)} \int_{[0,1]^d} a(x) ||\nabla u(x) + \xi||_{2}^{2} \mathrm{d}x.
\label{problem:effective_conductance}
\end{equation}
Now, for the second problem, we want to know the ground state energy of an electron along with its spatial distribution. Ground state energy is the state which electron wants to be in when the temperature drops to zero. Mathematically, it is the smallest eigenvalue of the problem of the form
\begin{equation}
\label{general_Shrodinger_equation_form}
Hu = Eu,
\end{equation}
which in case of multi electrons, is
\begin{equation}
Eu(r) := [-\Delta + v(r) + \int w(r-r^*) |u(r^*)^2|dr^*]u(r). 
\end{equation}
In \eqref{general_Shrodinger_equation_form}, $H$ is a Hamiltonian operator which is the sum of the kinetic energies of all the particles, plus the potential energy of the particles associated with the system.
Considering multiple particles, our second equation
\begin{equation}\label{NLSE}
-\Delta u(x) + a(x)u(x) + \sigma u(x)^3 = E_0 u(x), ~ x\in [0,1]^d, ~s.t. \int_{[0,1]^d}u(x)^2 \mathrm{d}x = 1,
\end{equation}
can be derived from \eqref{general_Shrodinger_equation_form}. In \eqref{NLSE}, $-\Delta u(x)$ represents the kinetic energy, $a(x)$ is the potential.

The main contributions of this work are:
\begin{enumerate}[i.]
	\item Providing theoretical guarantees on the neural network representation of $f(a)$ in \eqref{regression_key_task} through explicit construction for the parametric PDE problems under study;
	\item Showing that even a rather simple neural network architecture can learn a good representation of $f(a)$  in \eqref{regression_key_task} through training. 
\end{enumerate}

\section{Results}
\subsection{Effective Conductance in Inhomogeneus Media}
In 1D, mean squared error measured to be $2.5793 \times 10^{-5}$ on the training set and $5.20 \times 10^{-6}$ on test set. Finally, the predicted effective conductance by the network was $0.76800650$ with the error $\norm{mean(\mathscr{A}_{\text{eff-predicted}}) - mean(\mathscr{A}_{\text{eff-exact}})} = 1.02100 \times 10^{-3}$.

\subsection{Nonlinear Shr\"{o}dinger Equation}
In 2D, the mean squared error measured to be $0.0173$ on the training set and $0.01425044$ on the test data set. Finally, the mean of predicted ground state energy by the network was $10.17474556$ which has $7.235 \times 10^{-5}$ $L^2$-distance from the mean of the normalized ground state energy provided.

\section{Thesis Structure}

The rest of this thesis is structured as follows. In Chapter 2, the mathematical preliminaries are presented. In Chapter 3, we propose our discretization and neural model for the problems. In Chapter 4 we use the proposed model in Chapter 3 to compute the solution and report the results and conclude.












