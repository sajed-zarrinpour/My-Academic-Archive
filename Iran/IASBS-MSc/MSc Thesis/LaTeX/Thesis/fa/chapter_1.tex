\section*{مقدمه}
\addcontentsline{toc}{section}{مقدمه}
مقدارسنجی عدم قطعیت (۱۵) در فیزیک و مهندسی اغلب شامل مطالعه معادلات دیفرانسیل با مشتقات جزئی با میدان ضرایب تصادفی است. برای درک رفتار یک سیستم شامل عدم قطعیت، می‌توان کمیت‌های فیزیکی مشتق شده از معادلات دیفرانسیل توصیف کننده آن سیستم را به عنوان توابعی از میدان ضرایب استخراج کرد. اما حتی با گسسته‌سازی مناسب روی دامنه معادله و برد متغیرهای تصادفی، این کار به طور ضمنی به حل عددی معادله دیفرانسیل با مشتق جزئی به تعداد نمایی می‌انجامد.\\
یکی از روش‌های متداول برای مقدار سنجی عدم قطعیت، روش نمونه‌ برداری مونته کارلو است. گرچه این روش در بسیاری از موارد کاربردی است اما کمیت اندازه‌گیری شده ذاتاً دارای نویز است. به‌علاوه، این روش قادر به پیدا کردن جواب‌های جدید در صورتی که قبلا نمونه‌گیری نشده باشند، نیست. ما به‌دنبال یافت روشی هستیم که نویز داده‌ها در جواب آن تأثیر چندانی نداشته باشند و همچنین، قادر به ارائه جواب برای حالاتی که قبلا نمونه گیری نشده باشند نیز باشد. \\
روش گالرکین تصادفی با استفاده چند جمله‌ای‌های آشوب  یک جواب تصادفی را روی فضای متغیرهای تصادفی بسط می‌دهد و به این طریق مسئله با بعد بالا را به تعدادی معادله دیفرانسیل با مشتقات جزئی معین تبدیل می‌کند. این گونه روش‌ها به دقت زیادی درباره تعیین توزیع عدم قطعیت نیازمند هستند و از آن‌جا که پایه‌های استفاده شده مستقل از مسئله هستند، وقتی بعد متغیرهای تصادفی بالا باشد هزینه محاسباتی بسیار زیاد خواهد شد. ما به‌دنبال یافتن راهی برای حذف نیاز به محاسبه این پایه‌ها به صورت سنتی هستیم. یکی از اهداف این پایان‌نامه مطالعه نقطه ضعف روش‌های عددی سنتی همانند روش گالرکین تصادفی و روش المان‌های متناهی است. برای این منظور، تئوری روش المان‌های متناهی را مورد مطالعه دقیق‌تر قرار می‌دهیم.\\
هدف کار ما پارامتری کردن جواب یک معادله دیفرانسیل معین به کمک شبکه‌های عصبی (ساخت نمایشی دیگر برای جواب بر پایه ترکیب توابع) و سپس استفاده از روش‌های بهینه‌سازی برای یافتن جواب معادله است. در این پایان‌نامه تابع مورد نظر برای پارامتری‌سازی روی میدان ضرایب معادله دیفرانسیل با مشتقات جزئی تعریف شده است. در واقع ما به دنبال کاهش بعد مبتنی بر نمایش شبکه ‌عصبی برای حل معادلات دیفرانسیل با مشتقات جزئی همراه با عدم قطیت هستیم.
\section*{انگیزه و هدف}
\addcontentsline{toc}{section}{انگیزه و هدف}
مدل‌سازی طبیعت همیشه با پارامترهایی همراه است که مقادیر آنها از کنترل ما خارج است. اما عموما ما درباره محدوده تغییرات این پارامترها اطلاعاتی داریم. به معادلاتی که شامل این‌گونه پارامترها هستند، معادلات با ضرایب عدم قطعیت گوییم. به طور مثال، در مورد حرکت نفت در سفره‌های زیر زمینی؛ برای بیان شیوه حرکت مایعات نیاز به دانستن مکان حفره‌ها  در بافت سفره زیر زمینی داریم. این امر را می‌توان به صورت رسانایی مؤثر در حضور ناخالصی نیز در نظر گرفت. به عنوان مثالی دیگر، مسئله‌ای را مطرح می‌کنیم که نقطه شروع این رساله بوده است.\\ 
برای تشخیص سرطان پستان روش‌های متعددی موجود است. از جمله آن‌ها می‌توان به تصویر برداری پستان با بازتابش اشعه ایکس (XRM)\LTRfootnote{Projection X-ray mammography}، تصویربرداری با استفاده از ارتعاشات مغناطیسی (MRI)\LTRfootnote{Magnetic resonance imaging}، تصویربرداری فراصوت (US)\LTRfootnote{Ultra sound}، توموسنتز دیجیتال (DBT)\LTRfootnote{Digital breast tomosynthesis}، ماموگرافی انتشار پوزیترون (PET)\LTRfootnote{Positron emission mammography} و توموگرافی فراصوت (UST)\LTRfootnote{Ultra sound tomography} اشاره کرد. هرکدام از این روش‌ها اطلاعات را به طرق مختلفی نمایش می‌دهند، به این معنا که غده‌ای که در یکی از این روش‌ها غیر قابل تشخیص است در روش دیگر قابل تشخیص است؛ غده‌ای که در یک روش بافت مشکوک معرفی می‌شود، در روش دیگر می‌تواند به عنوان غده‌ای سالم و طبیعی معرفی شود. و این موضوع باعث ایجاد مشکلات بسیاری در روند تشخیص و برنامه ریزی درمان می‌شود. در این مرحله، راه‌حلی که به ذهن می‌رسد، ترکیب نتایج حاصل از این روش‌ها برای بالابردن ضریب دقت است؛ لیکن مشکل دیگری مانع این‌کار می‌شود. بافت پستان بسیار کشسان است و به راحتی تغییر فرم می‌دهد. از طرفی در هرکدام از این روش‌ها بیمار به حالت خاصی قرار می‌گیرد که با روش دیگر متفاوت است. به طور مثال، طی MRI بیمار در حالت دمر قرار دارد ولی برای تصویربرداری فراصوت بیمار به پشت می‌خوابد. علاوه‌براین، در روش بایوپسی راهنمایی شده توسط MRI\LTRfootnote{MRI-guided biopsy} بافت پستان توسط صفحه‌های سخت و غیرقابل انعطافی بی حرکت می‌شوند که منجر به فشرده شدن بافت نیز می‌شود. بنابراین، شکل، اندازه و مکان غده در این تصاویر متفاوت خواهد بود. این امر مقایسه تصاویر را با سختی بسیار همراه می‌کند. علاوه‌براین، برای برنامه‌ریزی پیش از جراحی، پزشک نیاز به دانستن مکان و اندازه دقیق غده دارد. بنابراین، نیاز به توسعه الگوریتم‌های ثبت غیرسخت\LTRfootnote{Non-rigid registration algorithm} احساس می‌شود.\\
روش‌هائی مبتنی بر روش المان‌های متناهی\LTRfootnote{Finite element method} برای حل این مسئله ارائه شده‌اند. اما مشکل عمده این روش‌ها هزینه محاسباتی بالای آنها است. مطابق آن‌چه در (۷) گفته شده است، به طور متوسط اجرای  یک شبیه‌سازی صد و بیست دقیقه به طول می‌انجامد که برای مصارف کلینیکی مقرون به صرفه نیست. مارتینز و همکارانش (۷) در سدد ارائه روشی برای کاهش این هزینه محاسباتی با استفاده از شبکه‌های عصبی بودند. گرچه، مدل ارائه شده توسط آنها زمان محاسبات را به طرز چشمگیری کاهش می‌دهد، اما پارامترهای مدل مورد مطالعه آن‌ها ثابت است. به عبارت دیگر، با توجه به این‌که این پارامترها از بدنی به بدن دیگر متفاوت هستند، برای ارائه یک مدل مختص به بیمار در زمانی قابل قبول، نیاز به توسعه مدل برای فراگیری پارامترهای دارای عدم قطعیت است. به طور مثال، نیازمند در نظر گرفتن ضریب کشسانی بدن بیمار، که یک ضریب عدم قطعیت است، هستیم. از این رو، برآن شدیم که بدنبال حل عددی معادلات دیفرانسیل (بیضوی) به کمک شبکه‌های عصبی باشیم.\\ 
%در این رساله، بدنبال حل عددی معادلات دیفرانسیل با مشتقات جزئی بیضوی خطی و غیر خطی ناهمگن هستیم. روشی که ما در صدد گزارش آن هستیم، یک روش کاهش بعد برای محاسبه جواب بدون نیاز به حل مستقیم معادله دیفرانسیل است. به عبارت دیگر، در روش‌های عددی متدوال، فرد بدنبال پیدا کردن تقریبی از جواب در یک فضای متناهی یا غیر متناهی است که در حالت متناهی به صورت ترکیب خطی از پایه هایی نوشته می‌شوند که در روند روش روی نقاط رأسی بدست می‌آیند. ما به‌دنبال راهی برای حذف ساخت این پایه‌ها به کمک شبکه عصبی هستیم و به این وسیله در پی کاهش بعد فضای جواب با حفظ خصوصیات اصلی مورد نیاز خود در جواب هستیم. با توجه به اینکه نمایش جواب بدست آمده از شبکه‌های عصبی به صورت ترکیبی متناهی از توابع بدست می‌آید می‌توانیم با تعویض نمایش جواب از حالت خطی (روش‌های عددی متداول) با حالت غیر خطی (نمایش شبکه‌های عصبی) به این مهم دست یابیم. ایده، استفاده از شبکه‌های عصبی برای یادگیری نگاشتی از دامنه ضرایب عدم قطعیت به فضای جواب بر اساس مجموعه داده‌ای از قبل محاسبه شده است.
در روش‌های عددی متدوال، هدف یافتن تقریبی از جواب مدل در یک فضای متناهی یا غیر متناهی است که به صورت یک ترکیب خطی از پایه های آن فضا در نقاط رأسی بدست می‌آید. دقت این تقریب با تعداد نقاط رأسی به کار رفته رابطه مستقیم دارد. بنابراین، محاسبه تقریبی دقیق برای مسائل پیچیده روی دامنه‌های پیچیده، همچون محاسبه میزان تغییر فرم بافت پستان در اثر نیروهای وارده، به ناچار مستلزم استفاده از تعداد بسیار زیادی نقاط رأسی روی دامنه مدل می‌باشد که به طور مستقیم منجر به افزایش زمان محاسبه جواب مدل می‌شود. از این‌رو  با تکیه بر اصول شبکه‌های عصبی بدنبال راهی برای حذف نیاز به محاسبه مستقیم پایه‌های فضای تقریب روی نقاط رأسی با استفاده از روشی مبتنی بر کاهش بعد فضای جواب با حفظ خصوصیات اصلی مورد نیاز خود از جواب هستیم. با توجه به اینکه نمایش جواب بدست آمده از شبکه‌های عصبی به صورت ترکیبی متناهی از توابع است، می‌توانیم با تعویض نمایش جواب از حالت خطی (روش‌های عددی متداول) با حالت غیر خطی (نمایش شبکه‌های عصبی) به این مهم دست یابیم. ایده، استفاده از شبکه‌های عصبی برای یادگیری نگاشتی از دامنه ضرایب عدم قطعیت به فضای جواب بر اساس مجموعه داده‌ از قبل محاسبه شده است. به این ترتیب می‌توان گفت روشی که ما در صدد گزارش آن هستیم، یک روش کاهش بعد برای محاسبه جواب بدون نیاز به حل مستقیم معادله دیفرانسیل است. در این رساله، معادلات دیفرانسیل با مشتقات جزئی بیضوی خطی و غیر خطی ناهمگن را مورد بررسی قرار می‌دهیم. نکته حائز اهمیت این است که از آن‌جا که این روش، یک روش مبتنی بر داده است، نوع مدل (بیضوی، هذلولوی و یا سهموی) یا روش عددی استفاده شده برای نمونه‌گیری جواب و ایجاد پایگاه داده جدید، در آن تغییر چشمگیری ایجاد نمی‌نمایند. بنابراین، این روش برای هر سه نوع معادلات دیفرانسیل با مشتقات جزئی در هردو حالت همگن و ناهمگن قابل استفاده است. از طرف دیگر، از آنجا که ما از محاسبات تنسوری در لایه‌های این شبکه بهره می‌بریم، این مدل با کمترین میزان تغییرات قابل استفاده در ابعاد بالاتر نیز است. برای نمایش این ادعا، معادله اول را در یک بعد و معادله دوم را در دو بعد بررسی خواهیم کرد.
\section*{مقدارسنجی عدم قطعیت }
\addcontentsline{toc}{section}{مقدارسنجی عدم قطعیت }
مقدارسنجی عدم قطعیت مطالعه اعتمادپذیری نتیجه‌گیری علمی است. به عبارت دیگر به جای مطالعه درستی هر فرضیه به طور جزئی، به دنبال  مطالعه ارتباط بین فرضیات هستیم. سؤال اساسی در مسائل مقدار سنجی عدم قطعیت این است که با فرض اینکه اطلاعاتی درمورد خطا در ورودی‌های مدل در دسترس باشد، در مورد تغییرات در خروجی چه می‌توان گفت؟(۱۵) برای مقدار سنجی عدم قطعیت یک کمیت فیزیکی، یکی از راه‌ها پیدا کردن نگاشتی از فضای پارامترهای عدم قطعی در ورودی مدل به فضای خروجی مدل است. خوشبختانه این نگاشت‌ها عموما وابسته به تعداد محدودی 'خصیصه` هستند که می‌توان آنها را با تعداد محدودی نمونه‌برداری و حل مدل دیفرانسیلی به‌دست آورد.\\
عموما عدم قطیت ناشی از فرضیات ساختار مدل و یا مقدار وردی‌های آن است. در این پایان‌نامه، ما روی عدم قطعیت حاصل از عدم قطعیت روی ورودی‌های مدل تمرکز خواهیم کرد. عدم قطعیت روی داده‌های ورودی به چهاردسته پارامترهای تصادفی، میدان‌های تصادفی، نویز سفید و همبسته طبقه‌بندی می‌شود (۱۶). در مسئله یافتن ضریب رسانایی مؤثر پر محیط ناهمگون عدم قطعیت روی داده های ورودی از نوع پارامترهای تصادفی و در مسئله تعیین سطح انرژی حالت پایه با پتانسیل زمینه ناهمگون از نوع نویز سفید هستند. \\
اغلب مقدار خروجی مورد نظر ما مقداری است که از فراوری‌ داده‌های خروجی مدل اولیه حاصل می‌شود. دراین‌جا نیز، هدف ما در واقع کنترل عدم قطعیت خروجی ثانویه مدل است که از فراوری حاصل می‌شود. با فرض اینکه عدم قطعیت ورودی مدل با بردار $\bm{a}=(a_1,\dots,a_N)^T$ پارامتری شده باشد، جواب معادله علاوه بر ابعاد فضایی $\bm{x}$ و بعد زمان $\bm{t}$ به بردار پارامترهای رندوم $\bm{a}$ نیز وابسته است. بنابراین یک نمونه‌گیری از جواب می‌تواند به صورت $u(x,t;a)$ به ازای یک انتخاب مشخص از پارامترهای رندوم بیان شود. گرچه ما علاقه‌‌مند به بررسی هر کدام از این نمونه‌ها به طور مجزا نیستیم (۱۶). بنابراین ما پایگاه داده خود را بر همین اساس خواهیم ساخت. کنترل عدم قطعیت مقدار خروجی مطلوب معمولا مستلزم تکامل یک انتگرال چندگانه روی بردار پارامتری $\bm{a}$ است (۱۶). مدل مطرح شده در این پایان‌نامه، این تکامل و تقریب عددی جواب آن را به طور همزمان بر عهده دارد. بدین منظور، به جای تلاش برای یافت جواب مدل، در سدد یافت نگاشتی از پارامترهای تصادفی ورودی به خروجی پردازش شده مدل هستیم. دقت کنید که به همین دلیل، با اینکه ما روش خود را روی معادلات بیضوی بررسی کردیم، همین روش برای معادلات هذلولوی و سهموی نیز کاراست.
\section*{تعریف مسئله }
\addcontentsline{toc}{section}{تعریف مسئله}
در این پایان نامه، هدف ما بررسی یک مدل میانبر برای حل مسائل معادلات دیفرانسیل با مشتقات جزئی به کمک شبکه‌های عصبی است. به عبارت روشن‌تر یافتن نگاشتی از فضای عدم قطعیت مسئله به فضای جواب. مسائلی که در این رساله برای حل انتخاب شده اند از این جهت حائز اهمیت بوده‌اند که هر دو حالت خطی و غیر خطی، معادلات دیفرانسیل غیر همگن شامل عدم قطعیت را پوشش می‌دهند.
\subsection*{یافتن ضریب رسانایی مؤثر در محیط ناهمگون}
معادله اول، رسانایی مؤثر در یک جهت انتخاب شده درون یک محیط غیر همگون را توسط ضریب رسانش توصیف می‌کند. محیط غیر همگون، محیطی است که در آن خصوصیات مورد توجه در تمامی نقاط یکسان نیستد. این امر ممکن است به دلایلی همچون جنس‌های گوناگون مواد تشکیل دهنده یا چگالی های متفاوت مربوط باشد. فرض ما بر آن است که ضریب رسانایی ماده در جهات متفاوت یکسان نباشد و این ضریب را با $\bm{a}(x)$ نمایش می‌دهیم. این بردار از نمونه‌گیری توابعی به فرم زیر روی نقاط رأسی حاصل می‌شود:
\begin{equation}
	\mathscr{A} = \{a\in L^{\infty}([0,1]^d) | \lambda_{1} \geq a(x) \geq \lambda_{0} > 0 \},
\end{equation}
که در آن $\lambda_{0}$ و $\lambda_{1}$ اعداد ثابتی هستند.با فرض انتخاب یک جهت دلخواه ثابت $\xi \in \mathbb{R}^d$، میزان ضریب رسانش در آن جهت مطلوب است. به عبارت دقیق‌تر، جواب معادله زیر مد نظر است
\begin{equation*}
A_{\text{eff}}(\bm{a}) = \min_{u(x)} \int_{[0,1]^d} \bm{a}(x) ||\nabla u(x) + \xi||_{2}^{2} \mathrm{d}x.
\end{equation*}
این معادله با فرم دیفرانسیلی زیر هم‌ارز است. به عبارت دیگر، جواب مسئله بهنه‌سازی فوق در معادله دیفرانسل زیر سدق میکند و جواب معادله دیفرانسیل زیر نیز تابع هدف مسئله بهینه‌سازی فوق را بهینه می‌کند.
\begin{equation}
	-\nabla \cdot (a(x)(\nabla u(x)+\xi)) = 0 
\end{equation}
ازاین‌رو، با جای حل مسئله مینیم‌سازی، فرم مذکور را به کمک گسسته‌سازی زیر با گام $h=\frac{1}{n}$ حل می‌نماییم.
\begin{multline*}
	-\sum_{k=1}^{d} \frac{a_{i+\frac{1}{2}e_{k}} - a_{i-\frac{1}{2}e_{k}}}{h} \cdot \frac{u_{i+e_{k}} - u_{i}}{h} - \sum_{k=1}^{d} a_{i-\frac{1}{2} e_k}  \frac{u_{i+e_{k}} - 2 u_{i} + u_{i-e_{k}}}{h^2} - \sum_{k=1}^{d} \xi_k \frac{a_{i+\frac{1}{2}e_{k}} - a_{i-\frac{1}{2}e_{k}}}{h} \\
	= - \sum_{k=1}^{d} \frac{ a_{i+\frac{1}{2}e_{k}} u_{i+e_k} - a_{i+\frac{1}{2}e_{k}} u_i - a_{i-\frac{1}{2}e_{k}} u_{i+e_k} + a_{i-\frac{1}{2}e_{k}} u_i + a_{i-\frac{1}{2}e_{k}} u_{i+e_k} - 2 a_{i-\frac{1}{2}e_{k}} u_i + a_{i-\frac{1}{2}e_{k}} u_{i-e_k}}{h^2} \\ - \sum_{k=1}^{d} \xi_k \frac{a_{i+\frac{1}{2}e_{k}} - a_{i-\frac{1}{2}e_{k}}}{h} \\
	= - \sum_{k=1}^{d} \frac{ a_{i+\frac{1}{2}e_{k}} [u_{i+e_k} - u_i] -  a_{i-\frac{1}{2}e_{k}} [u_i - u_{i-e_k}]}{h^2} - \sum_{k=1}^{d} \xi_k \frac{a_{i+\frac{1}{2}e_{k}} - a_{i-\frac{1}{2}e_{k}}}{h}  \\
	= \sum_{k=1}^{d} \frac{- a_{i+\frac{1}{2}e_{k}} u_{i+e_k} + [a_{i+\frac{1}{2}e_{k}} + a_{i-\frac{1}{2}e_{k}}] u_i - a_{i-\frac{1}{2}e_{k}} u_{i-e_k}}{h^2} - \sum_{k=1}^{d} \xi_k \frac{a_{i+\frac{1}{2}e_{k}} - a_{i-\frac{1}{2}e_{k}}}{h} = 0.
\end{multline*}
که می‌تواند به صورت $(L_a U)_i = (b_a)_i$ نمایش داده شود که در آن 
\begin{align}
	\label{def:L_a}
	(L_a u)_i &:= \sum_{k=1}^{d} \frac{-a_{i+\frac{1}{2}e_k} u_{i+e_k} + (a_{i-\frac{1}{2}e_k} + a_{i+\frac{1}{2}e_k})u_i - a_{i-\frac{1}{2}e_k} u_{i-e_k} }{h^2}\\
	\label{def:b_a}
	(b_a)_i &:= \sum_{k=1}^{d} \frac{\xi_k (a_{i+\frac{1}{2}e_k} - a_{i-\frac{1}{2}e_k})}{h} .
\end{align}
همان‌طور که اشاره کردیم، قصد ما حل این مسئله در حالت یک بعدی است. بنابراین در روابط بالا $d=1$ قرار می‌دهیم. با درنظر گرفتن شرط مرزی $u_{\vert_{\partial\Omega}} = 0$، در مرحله بعد اقدام به نوشتن فرم ماتریسی آن می‌نماییم.
\begin{equation*}
	L_{a} = \frac{1}{h^2}\begin{bmatrix}
		a_{1 - \frac{1}{2}e_{1}} + a_{1 + \frac{1}{2}e_{1}}  &  -a_{1+\frac{1}{2}e_{1}} &  &  &  \\
		-a_{2-\frac{1}{2}e_{1}}&a_{2 - \frac{1}{2}e_{1}} + a_{2 + \frac{1}{2}e_{1}}& -a_{2 + \frac{1}{2}e_{1}} & & \\
		& \ddots& \ddots &\ddots  &  \\
		& & -a_{n-1-\frac{1}{2}e_{1}}&a_{n-1 - \frac{1}{2}e_{1}} + a_{n-1 + \frac{1}{2}e_{1}}&-a_{n-1 + \frac{1}{2}e_{1}}\\
		& & & -a_{n-\frac{1}{2}e_{1}} &a_{n-\frac{1}{2}e_{1}} + a_{n+\frac{1}{2}e_{1}}
	\end{bmatrix}_{n \times n},
\end{equation*}
\begin{equation*}
	b_a = \frac{1}{h}\begin{bmatrix}
		\xi_1 (a_{1+\frac{1}{2}e_{1}} - a_{1-\frac{1}{2}e_{1}}) \\
		\xi_1 (a_{2+\frac{1}{2}e_{1}} - a_{2-\frac{1}{2}e_{1}}) \\
		\vdots \\
		\xi_1 (a_{n+\frac{1}{2}e_{1}} - a_{n-\frac{1}{2}e_{1}})
	\end{bmatrix}_{n \times 1},
	U = \begin{bmatrix}
		u_{1} \\
		u_{2} \\
		\vdots \\
		u_{n}
	\end{bmatrix}_{n \times 1}.
\end{equation*}
با تعاریف بالا جواب را به صورت $L_a U = b_a$ می‌توان نوشت. اما برای رسیدن به گسسته‌سازی نهایی، هنوز نیاز داریم که مسئبه را با در نظر گرفتن شرط مرزی دوره‌ای بازنویسی نماییم.
\begin{equation*}
	L_{a}^{\text{patched}} = \frac{1}{h^2}\begin{bmatrix}
		a_{1 - \frac{1}{2}e_{1}} + a_{1 + \frac{1}{2}e_{1}}  &  -a_{1+\frac{1}{2}e_{1}} &  &  & & &-a_{1-\frac{1}{2}e_{1}}\\
		&& \ddots& \ddots &\ddots  &&  \\
		-a_{n+\frac{1}{2}e_{1}}&& & & -a_{n-\frac{1}{2}e_{1}} &a_{n-\frac{1}{2}e_{1}} + a_{n+\frac{1}{2}e_{1}}
	\end{bmatrix}_{n \times n},
\end{equation*}
\begin{equation*}
	U = \begin{bmatrix}
		u_{1} \\
		u_{2} \\
		\vdots \\
		u_{n}
	\end{bmatrix}_{n \times 1},
\end{equation*}
که در آن $a_{1-\frac{1}{2}e_{1}} = \frac{a_{n} + a_{1}}{2}$ و $a_{n+\frac{1}{2}e_{1}} = \frac{a_{1} + a_{n}}{2}$. نهایتا پس از حل مسئله توسط یک روش تکراری با گسسته سازی مطرح شده، جواب بدست آمده را در رابطه زیر قرار می‌دهیم تا به رسانایی مؤثر دست یابیم
\begin{equation}
	A_{\text{eff}}(a) = h^d (u_{a}^{T} L_{a}^{\text{patched}} u_{a} - 2u_{a}^{T}b_a + a^T 1).
\end{equation} 
\subsection*{تعیین سطح انرژی حالت پایه با پتانسیل زمینه ناهمگون}
معادله دوم، معادله غیرخطی شرودینگر دو بعدی است. هدف از این معادله، یافتن میزان انرژی حالت پایه الکترون با پتانسیل اولیه همراه با عدم قطعیت است. حالت پایه سطح انرژی‌ای است که الکترون مایل به اخذ آن در دمای صفر مطلق می‌باشد. این معادله به صورت یک مسئله مقدار ویژه به فرم زیر تعریف می‌شود. هدف در حل این مسئله یافتن کوچکترین مقدار ویژه آن (حالت پایه) است
\begin{align*}
-\Delta u(x) + a(x)u(x) + \sigma u(x)^3 =& E_0 u(x)\\ x\in [0,1]^d, s.t. \int_{[0,1]^d}u(x)^2 dx =& 1.
\end{align*}
مشابه آنچه در قسمت قبل آمد معادله فوق را گسسته می‌کنیم
\begin{equation}
	(Lu)_i + a_i u_i + \sigma u_{i}^3 = E_0 u_i, \sum_{i=1}^{n^d} u_{i}^2 h^d = 1
\end{equation}
که در آن
\begin{equation*}
	(lu)_i := \sum_{k=1}^{d} \frac{-u_{i+e_{k}} + 2u_i - u_{i-e_{k}}}{h^2}.
\end{equation*}
\section*{شبکه‌های عصبی }
\addcontentsline{toc}{section}{شبکه‌های عصبی }
شبکه عصبی\LTRfootnote{Neural network} از تعدادی واحد متصل به هم نام نورون \LTRfootnote{Neuron} تشکیل می‌شود. هر نورون دارای یک وضعیت داخلی است که در ترکیب با داده ورودی تغییر می‌کند و توسط تابع فعال‌سازی\LTRfootnote{Activation function} خروجی نورون را به حالت روشن یا خاموش تغیر می‌دهد. به عبارت ریاضی، هر نورون دارای اسکالرهای داخلی به نام وزن\LTRfootnote{Weight} و بایاس\LTRfootnote{Bias} و تابع فعالسازی‌ای است که به ترتیب با $W$ و $b$ و $\phi$ نمایش داده می‌شوند. و خروجی نورون در این صورت با فرض اینکه $X$ ورودی نورون باشد به صورت $\phi(WX + b)$ خواهد بود. برای این ساختار تابعی به عنوان تابع انحراف\LTRfootnote{Loss function} به عنوان تابع هدف تعریف می‌شود که در حالت یادگیری تحت نظارت\LTRfootnote{Supervised learning} به عنوان معیاری برای تعیین انحراف جواب‌های شبکه از جواب‌های واقعی معرفی شده به شبکه است. در این صورت، می‌توان روند یادگیری یک شبکه عصبی را معادل با یک مسئله کمینه‌سازی برای کمینه کردن میزان این تابع هدف در نظر گرفت. ابزار شبکه برای کمینه سازی این تابع هدف، تغییر وزن‌ها و بایاس‌های نورون‌های خود است. قضیه زیر که به قضیه تقریب جهانی مشهور است، شرایط استفاده از شبکه‌های عصبی برای تقریب جواب مسئله را بیان می‌کند.\\
\textbf{قضیه}. (قضیه تقریب جهانی) 
	\begin{enumerate}
	\item \textbf{(حالت نامتناهی)} فرض کنید $\varphi :\mathbb {R} \to \mathbb {R}$ یک تابع غیرثابت پیوسته بی‌کران باشد که آن را تابع فعال‌سازی می‌نامیم. فرض کنید $I_{m}$ بیان‌گر ابر مکعب $m$-بعدی $[0,1]^{m}$ باشد، و فضای توابع پیوسته حقیقی مقدار روی $I_m$ با $C(I_{m})$ نمایش داده شود. در این‌صورت، به ازای هر $\varepsilon > 0$ دلخواه و هرتابع  $f\in C(I_{m})$, ثوابت حقیقی مانند  $v_{i},b_{i}\in \mathbb  {R}$ و بردارهای $w_{i}\in \mathbb {R} ^{m}$ برای $i=1,\dots ,N$وجود دارند، به طوری‌که می‌توانیم  	
	\begin{equation*}
	F(x)=\sum _{{i=1}}^{{N}}v_{i}\varphi \left(w_{i}^{T}x+b_{i}\right)
	\end{equation*}
	را به عنوان تقریبی از $f$ ارائه دهیم:
	\begin{equation*}
	|F(x) - f (x)|  < \varepsilon
	\end{equation*}
	که در آن $x\in I_{m}$ است. به عبارت دیگر، توابع به شکل $F(x)$ در $C(I_{m})$ چگال‌اند.\\
	این نتیجه به ازای هر زیر مجموعه فشرده دیگری از $\mathbb {R} ^{m}$ به جای  $I_m$ نیز برقرار است.
	\item \textbf{(حالت کران‌دار)} در شبکه‌های کران‌دار، برای هرتابع انتگرال‌پذیر لبگ مانند  $f:\mathbb {R} ^{n}\rightarrow \mathbb {R}$ و هر $\epsilon >0$ یک شبکه ReLu\LTRfootnote{Rectified linear unit} کامل $\mathcal {A}$ با عرض $ d_{m}\leq {n+4}$, به گونه ای موجود است که $F_{\mathcal {A}}$ نمایش داده شده با این شبکه در رابطه
	\begin{equation*}
	\int _{\mathbb {R} ^{n}}\left|f(x)-F_{\mathcal {A}}(x)\right|\mathrm {d} x<\epsilon
	\end{equation*}
	صدق ‌نماید.
\end{enumerate}
\section*{روش پیشنهادی }
\addcontentsline{toc}{section}{روش پیشنهادی }
معادله تعیین ضریب رسانش مؤثر تنها در یک بعد، و معادله شرودینگر در دو بعد حل خواهند شد. هدف این انتخاب تأکید بر این نکته بوده است که با اندک تغییری در ساختار ورودی‌های شبکه، می‌توان از این مدل برای حل معادلات دیفرانسیل در هر بعد دلخواهی استفاده کرد. دلیل این امر نیز استفاده از محاسبات تنسوری در ساختار شبکه عصبی می‌باشد.\\
توجه خواننده را به این نکته جلب می‌نمائیم که روش عددی‌ای که برای ساخت پایگاه داده اولیه مورد نیاز برای شبکه عصبی استفاده می‌شود، از جهتی حائز اهمیت است؛ زیرا به میزانی که خطای ما روی داده اولیه کمتر باشد اطمینان ما از جواب شبکه عصبی بیشتر است. از طرفی، بررسی اینکه این خطا در داده‌های اولیه چگونه در شبکه عصبی منتشر می‌شود و شبکه تا چه میزان به خطا در داده های ورودی خود حساس است، از حیطه این پایان نامه خارج است. بنابراین، از آنجایی که شبکه عصبی صرفا از یک جدول داده بهره می‌برد و نسبت به اینکه این جدول داده‌ای از چه راهی بدست آمده اطلاع قبلی ندارد، در این پایان نامه فرض بر این است که روش عددی استفاده شده برای ایجاد پایگاه داده با شبکه عصبی غیر مرتبط است و بنابراین از هر روش عددی‌ای می‌توان استفاده نمود. به همین منظور از روش تفاضلات متناهی برای تولید پایگاه داده در هر دو مسئله استفاده نموده‌ایم.\\
برای حل معادلات با روش تفاضلات متناهی، معادله تعیین ضریب رسانش مؤثر روی یک شبکه نه نقطه‌ای متساوی‌الفاصله، با مقادیر ضرایب عدم قطعیت با توزیع نرمال $\mathcal{U}[0.3, 1.5]$، و معادله شرودینگر غیر خطی روی یک شبکه هشتاد و یک نقطه‌ای متساوی‌الفاصله (گسسته سازی نه نقطه‌ای هر کدام از ابعاد)با مقادیر ضرایب عدم قطعیت با توزیع نرمال $\mathcal{U}[1, 16]$ به تعداد نمونه‌های مورد نیاز حل می‌شوند. سپس درصدی از تکرارها (در اینجا هفتاد و پنج درصد) به عنوان داده برای مرحله آموزش و الباقی برای مرحله آزمون کنار گذاشته می‌شوند.\\
شبکه عصبی متشکل از سه بخش است. بخش اول و سوم قرینه یکدیگر و متشکل از لایه‌های پیچشی \LTRfootnote{convolutional layers}  هستند که به واسطه بخش دوم که یک استخر مجموع \LTRfootnote{sum-pooling} است به هم متصل شده‌اند. ورودی این شبکه برای رسانایی مؤثر یک بردار و برای معادله شرودینگر یک ماتریس است. دقت شود که در حالت دو بعدی، قبل از لایه‌های پیچشی، ابعاد داده ورودی گسترش می‌یابد. این امر با توجه به اینکه شرط مرزی مسئله دوره‌ای است، به صورت گسترش دوره‌ای انجام می‌شود. خروجی شبکه در هر دو حالت یک اسکالر است. که در مورد ضریب رسانائی مؤثر، این اسکالر برابر ضریب رسانائی مؤثر در جهت ثابت $\xi$ و در مورد معادله شرودینگر، برابر با سطح انرژی حالت پایه است.\\
شبکه پس از چندین بار مرور داده‌ها در انتها ضرایب خود را به گونه ای تنظیم می‌کند که تابع هدفی که به آن معرفی کرده‌ایم را کمینه نماید. وقتی تابع مذکور به میزان کمینه خود برسد می‌گوییم آموزش شبکه به اتمام رسیده است. از این پس می‌توانیم با خوراندن ورودی جدید به شبکه از آن برای یافتن جواب استفاده نماییم.\\
\section*{نتایج}
\addcontentsline{toc}{section}{نتایج}
مقادیر ضریب رسانایی مؤثر و انرژی حالت پایه به ترتیب $0.76800650$ و $10.17474556$ بدست آمده اند که خطای $L^2$ به ترتیب، عبارت اند از $1.02100 \times 10^{-3}$ و $7.235 \times 10^{-5}$. نمودار توزیع خطا بر حسب نمونه برای ضریب رسانایی مؤثر به شرح زیر است.
\begin{figure}[h!]
	{
		\centering
		\def\svgwidth{\columnwidth}
		\scalebox{.5}{\input{images/ECIM_bar.pdf_tex}}
		\caption{خطای مرتکب شده  روی مجموعه آزمون به تفکیک نمونه}
		\label{fig:ECIM_error_bar}
	}
\end{figure}
\\همچنین، نمودار مشابه برای انرژی حالت پایه نیز به شرح زیر است.
\begin{figure}[h!]
	{
		\centering
		\def\svgwidth{\columnwidth}
		\scalebox{.5}{\input{images/bar.pdf_tex}}
		\caption{خطای مرتکب شده  روی مجموعه آزمون به تفکیک نمونه}
		\label{fig:NLSE_error_bar}
	}
\end{figure}
همان‌گونه که مشاهده می‌شود، تعداد نمونه‌های مورد استفاده قرا گرفته برای آموزش ماشین در معادله شرودینگر بسیار کمتر از معادله ضریب رسانایی مؤثر است. دلیل این امر هزینه بالای محاسباتی برای محاسبات این معادله نسبت به معادله اول بوده است. دقت کنید که در این معادله، به دلیل وجود جمله غیر خطی، مجبور به استفاده از یک روش تکراری برای حل معادله هستیم. روش حل ما بر مبنای روش هموتوپی نیوتنی بوده است. همچنین دقت شبکه عصبی در معادله شرودینگر به مراتب کمتر از معادله ضریب رسانایی مؤثر است. دلیل این امر را می‌توان به دوعامل نسبت داد. عامل اول تعداد کمتر نمونه‌هاست که به آن اشاره شد و دلیل دوم، بزرگ بودن برچسب ها و کوچک بودن مقیاس فواصل آن‌هانسبت به اندازه خود برچسب‌ها. بهر روی، برای تعدیل این مشکل و بالا بردن دقت داده‌ها توسط روش نرمال‌سازی Z-Score نرمال شده‌اند.
\clearpage
\newpage
\section*{نتیجه‌گیری و کارهای پیش‌رو}
\addcontentsline{toc}{section}{نتیجه‌گیری و کارهای پیش‌رو}
همان‌گونه که از نتایج مشهود است، شبکه‌های عصبی توانایی بالایی در تقریب روابط پنهان مابین داده‌ها دارند. همچنین سادگی روش، آن را به یک روش در دسترس تبدیل می‌کند. ضمن اینکه پس از طی مرحله آموزش، شبکه عصبی قادر است جواب مسئله را تقریبا به طور آنی ارائه کند. یکی از محدودیت‌های شبکه‌های عصبی در مورد اندازه مقیاس ورودی‌ها است. به این معنی که در صورتی که برچسب‌ها بسیار بزرگ باشند یا با فاصله بسیار از هم، روند یادگیری با مشکل مواجه می‌شود. همان‌گونه که مشاهده می‌شود خطا در معادله شرودینگر به علت بزرگ بودن برچسب ها در مقایسه با ضرایب عدم قطعیت بیشتر است. ما از روش نرمال سازی Z-Score برای نرمال‌سازی استفاده نمودیم.\\
در ادامه این پایان نامه، سؤالات زیر می‌تواند مورد بررسی بیشتر قرار گیرد
\begin{itemize}
	\item چه روش‌های دیگری برای حل این مسئله موجود است؟
	\item آیا این نرمال سازی خود خطایی به مدل تحمیل می‌کند؟ کران این خطای تحمیلی چیست؟
	\item میزان حساسیت شبکه عصبی به خطا در داده‌های ورودی و همچنین الگوی انتشار خطا در آن به چه صورت است؟
	\item آیا این انتشار خطا با افزایش عمق شبکه و یا استفاده از ساختار های متفاوت اعم از توابع متفاوت برای توابع فعالسازی و همچنین نوع آرایش نورون ها ارتباطی دارد؟
\end{itemize}
\clearpage
\newpage