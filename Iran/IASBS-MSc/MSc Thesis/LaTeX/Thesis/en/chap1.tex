\chapter{Introduction}
\pagenumbering{arabic}
\setcounter{page}{1}
This chapter begins with introducing the basic concepts required for the rest of the thesis, such as uncertainty quantification and neural networks. It follows by introducing two partial differential equations that we want to solve. This chapter is concluded by an overview of the structure of the thesis. 
\section{Background \& Motivation}
\label{sec:Background_And_Motivations}
To model real-world phenomenons, one makes assumptions to simplify his model. More often than not, the hidden parameters that take part in the model are unknown. In many other cases, the exact value of the inputs is unclear. These are examples of what called uncertainty. Usually, one wants to quantify these uncertainties to control them, if possible. Often, models are consist of a single or a system of Partial Differential Equations (PDEs). Then it is beneficial to study methods to solve PDEs that contain uncertainties in their structures. However, incorporating uncertainty would increase both complexity and computational time.\\
Our interests come from modeling the mechanical behavior of soft tissue, breast in particular, under compression. There are multiple methods to imaging the tissue. For example, consider magnetic resonance imaging (MRI) and ultra sound (US).‬ Each imaging method uses a different wave length, and because the tissue under study is a composite tissue they will produce different results. This cause a lesion to be identify in one method as healthy and unhealthy in another. On the other hand, each imaging method requires a different patient position. Considering the elasticity of breast, it will deform in any of these positions differently. For example in MRI guided biopsy, the patient is in the prone position and the breast compress via two rigid plate, while in ultra sound the patient is in the supine position and the breast is under no extra pressure than the gravity. This cause the lesion to be identify in potentially different location and size and shape in each image. Then to produce a reliable result, one has to compare/combine the results of these different imaging methods.\\
Due to its importance in diagnosing and treatment of breast cancer, this problem is an exciting subject for many researchers \cite{khatam2015vivo, tanner2002comparison, han2011development, azar2002methods, azar2001deformable, del2008finite, martinez2017finite}. For example in \cite{del2008finite, martinez2017finite, azar2001deformable}, finite element method (FEM) is used for modeling. However, according to \cite{martinez2017finite} the main problem with  FEM is it's long computation time. They tried to use a machine learning FEM based method to solve the issue and they succeed in getting the result in clinical time. However, their model is not patient specific. In the effort to improve their work, we are after answering this question: `How can we make it both patient-specific and efficient to use in actual clinics?'.\\
The first challenge is that each body has different properties, e.g., elasticity. To our knowledge, there is no definite way to determine the exact value of these properties yet. Then they can be thought of as uncertain coefficients in the model.\\
Next challenge is that the results of such modeling must compute in clinical time. This can be addressed with the proper usage of neural networks as illustrated in \cite{martinez2017finite}. Indeed this notion, using neural network for enhancing numerical approximation, is not new \cite{geneva2020modeling, smaoui2004modelling, lagaris2000neural, beck2019machine, zhang2019quantifying, parisi2003solving}.% There are multiple ways to enhance numerical approximation with neural networks. As an example, for being saved from the curse of dimensionality, one may compute the solution of a PDE on a mesh, then feed that answer to a neural network and use it to find the solution on a finer mesh. Another may find a surrogate forward model which describes a map between the coefficient field to the quantities of interests rather than solving the PDE itself.\\ 
Hence, one can consider solving PDEs with uncertainties using neural networks as a key tool in patient specific modeling of the soft tissue deformation. In this thesis, we are interested in
\begin{enumerate}[i.]
	\item A better understanding of existing numerical methods bottlenecks which cause time consumption,
	\item Studying ways in which one can avoid those bottlenecks,
	\item Applying proposed method on different problems and see how much changes it needs in structure to adapt.
\end{enumerate}
Two PDEs studied to cover both linear (in 1D) and nonlinear (in 2D) cases.\\
It worth mentioning that the numerical method that generates the dataset for the neural network to learn, has not much of an importance in our study. It is true that the error in the initial dataset may has impact on the final result, but studying error propagation is not in this thesis boarders. Hence, the classic finite difference method (FDM) is used to generate the initial dataset. However, to achieve our first goal, which was to understand the reason behind time consumption of FEM in the breast modeling problem, the analysis of FEM is presented here. It is indeed useful because any other numerical method that has similar bottleneck is also doomed.
\section{Partial Differential Equations}
\label{sec:PDE}
The idea behind PDE is simple; to describe the variation of a physical quantity that depends on multiple physical variables, one has to find its partial variation with respect to each physical variable while keeping others constant. Then, summing up all the variations shall give the overall variation of that quantity. To write this in math, one needs a mathematical equation that involves those independent variables, the unknown function (which depends on those variables), and partial derivatives of the unknown function with respect to the independent variables which call a PDE. By definition, the order of a PDE is the order of the highest derivative involved. A solution (or a particular solution) to a PDE then is a function that turns it into an identity when substituted into the equation. A solution is called general if it contains all particular solutions of the equation concerned \cite{stavroulakis1999partial}.\\
PDEs have been used  to formulate the solution of physical problems involving functions of several variables mathematically such as the propagation of heat or sound, fluid flow, elasticity, electrostatics, electrodynamics, etc \cite{stavroulakis1999partial}.\\
If all terms of a PDE contain the dependent variable or its partial derivatives, then such a PDE is called inhomogeneous or homogeneous otherwise. The external forces to the PDE models usually apply through the inhomogeneous term.\\
One needs additional information about the initial state of the model or change of it over the boundary called conditions to find the unique solution of a PDE. There are two types of conditions, initial conditions, and boundary conditions. An initial condition expresses the value of the solution or its derivatives at an initial point in time. A boundary condition, on the other hand, defines the value of the solution or its derivatives at the boundary of the domain of the problem. Boundary conditions often categories into three categories; \textit{Dirichlet} boundary condition defines the value of the exact solution.  \textit{Neumann} boundary condition describes the values of the derivatives of the exact solution. Finally, \textit{Robin} boundary condition describes both the solution and the derivatives. Note that the Robin boundary condition can formulate as a linear combination of the Dirichlet and Neumann boundary conditions. 
\section{Uncertainty Quantification}
This section gives an overview of an aspect of this work that mentioned in Section \eqref{sec:Background_And_Motivations}, which is uncertainty quantification (UQ). We talk briefly about the uncertain parameters of our model, and we describe what the model intends to do with them.\\
UQ is the end to end the study of the reliability of scientific inference that means the study of relationships between pieces of information, not the `truth' of those information/assumptions \cite{UQIntro_Sullivan}. In other words, given information about uncertainties in the input of a PDE, UQ seeks to determine information about the uncertainties in its output \cite{UQIntroGunzburger}. In that regard, one can extract PDE derived physical quantities as functionals of their random coefficient field. This can potentially impose the need to solve the PDE an exponential number of times. Fortunately, often these functionals depend only on a few characteristic `features' of the coefficient field that can be determined from solving PDE a limited number of times \cite{Base_paper}.\\
Uncertainty is either from an inherently variable phenomenon known as `aleatoric' or emerges from the lack of knowledge known as `epistemic'. Epistemic uncertainty concerns the correctness of the structure of the model itself while aleatoric uncertainty concerns the correct values of the parameters of the model \cite{UQIntro_Sullivan}. It is clear then that in this thesis, uncertainties are assumed to be aleatoric.\\
There are four major types of uncertain inputs: random parameters, random fields, white noise, and correlated. Random parameters are a finite number of random numbers each may vary independently according to its own given one-dimensional probability distribution function (PDF) or according to a given joint multivariate PDF collected in a vector called a random vector. On the other hand, random fields are defined over the spatial/temporal domain which means they represent values that vary randomly from one point to another and/or from one time instant to another each of which is determined by sampling from a single PDF (they are identically distributed). In the case of white noise in addition to being identically distributed, the value of the field at a point $x$ and time $t$ is independent of its value at all other spatial points and time instants (they are independent and identically distributed (i.i.d.) and hence uncorrelated). Finally, colored or correlated random fields are identically but not independently distributed  \cite{UQIntroGunzburger}. In this thesis, the uncertain inputs are considered to be random parameters and white noise respectively for our first and second problems.\\
Often, the output(s) obtained by post-processing the solution of the PDE is more interesting than the solution of the PDE itself. Hence, one can say that provided enough information about the uncertainty in the inputs of the PDE, a UQ task is to determine information about the uncertainty in the output of interest that depends on the solution of a PDE \cite{UQIntroGunzburger}.\\
In our first case, as discussed in Section \eqref{section_ECIM}, after solving the PDE, a post-processing procedure will apply to obtain an effective coefficient in the media. In the second case, the value of having a surrogate model is more significant due to the nonlinear cubic term, which makes it more difficult to solve numerically. In this case, we are interested in the smallest eigenvalue of the equation.\\
Assuming random inputs parameterized by the random parameters $\bm{a} = (a_1,\dots,a_N)^T$, the solution of the PDE is not only a function of spatial position $\bm{x}$ and/or time $t$, but also a function of the random parameter vector $\bm{a}$. Then, a realization of a solution $u(x,t; a)$ of a PDE is a solution obtained for a specific choice for the random parameters. However, there is no interest in individual realizations in general \cite{UQIntroGunzburger}. This is a hint on how we are going to create the datasets to train and test our model. As we mentioned earlier, the determination of the quantity of interest (which can be either the output of interest derived from the solution of the PDE through some post-processing or the solution of the PDE depend on the situation), often requires the evolution of a multidimensional integral for parameters $a_1,\dots,a_N$. Quadrature rules are used to approximate such integrals since usually they cannot be evaluated exactly \cite{UQIntroGunzburger}. Our model encapsulates the evolution and do its approximation at the same time. To get a glimpse of the idea, consider that instead of using methods that require the evolution of the quantity of interest for many choices of the parameter vector they depend on to determine statistical information, one can use relatively few solutions of the PDE to build a surrogate model, e.g. an interpolant or a reduced order model, for either the PDE itself or for the quantity of interest derived from the PDE. Indeed, the model constructed in this thesis is a surrogate model which is a map from the coefficient field $\mathscr{A}$ to the solution space. If the reader is more interested in solving PDE directly with a neural network, we refer him/her to the deep Ritz method (DRM) which uses the self-adjoint problem \cite{weinan2018deep} and the physical informed neural networks (PINN) which tend to create constraints derived directly from the PDE itself in the form of its loss function \cite{raissi2017physics,raissi2018deep}.
There is a tiny note to mention here, that though our examples are elliptic PDEs our model can be used for any type of PDE since it is a surrogate model.
\section{Artificial Neural Network}
In 1943, Warren McCulloch and Walter Pitts \cite{mcculloch1943logical} wrote a paper on how neurons might work. They modeled a simple neural network with electrical circuits. Although the study of the human brain is thousands of years old, this was the beginning of a new field that evolve into what we know today as artificial neural networks (ANN).\\
The short history of the development of deep neural networks goes as follow. ``In the late 1940s, D.O. Hebb created a learning hypothesis that became known as Hebbian learning \cite{hebb1949organization}. The first functional networks with many layers called `Group Method of Data Handling' were created by Ivakhnenko and Lapa in 1965 \cite{schmidhuber2015deep, ivakhnenko1973cybernetic, ivakhnenko1967cybernetics}. 
The basics of continuous backpropagation \cite{schmidhuber2015deep, dreyfus1990artificial, mizutani2000derivation} were derived in the context of control theory by Kelley \cite{kelley1960gradient} in 1960 and by Bryson in 1961, using principles of dynamic programming \cite{bryson1961gradient}. In 1970, Seppo Linnainmaa published the general method for automatic differentiation (AD) of discrete connected networks of nested differentiable functions \cite{linnainmaa1970representation, linnainmaa1976taylor}. Geoffrey Hinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine \cite{smolensky1986information} to model each layer. In 2012, Ng and Dean created a network that learned to recognize higher level concepts, such as cats, only from watching unlabeled images \cite{le2013building}. Unsupervised pre-training and increased computing power from graphics processing units (GPU) and distributed computing allowed the use of larger networks, particularly in the image and visual recognition problems, which became known as "deep learning".\\
Neural Networks, as we know them today, are network or circuit of real biological neurons or their artificial counterparts. In the latter case, we call it artificial neural network. In this thesis, we are working strictly on ANN and we might use the term neural network (NN) for simplicity.\\ 
ANNs are composed of artificial neurons that hold the biological concept of neurons that are receiving input, combine it with their internal state and turn on or off. They do that using weights, bias and activation function.\\
The weights have used to model the connections of the biological neurons. A \textbf{weight} is a `parameter' associated with a connection from one neuron $M$, to another neuron $N$. It corresponds to a synapse in a biological neuron, and determines how much notice the neuron $N$ pays to the activation it receives from neuron $M$ \cite{MLDict}.\\ %If the weight is positive, the connection is called excitatory, while if the weight is negative, the connection is called inhibitory'' \cite{MLDict}.\\
In `feedforward' and some other neural networks, each hidden unit, and each output unit is connected via a trainable weight to a unit (the \textbf{bias} unit) that always has an activation level of –1. This gives a trainable threshold equal to the value of the weight from the bias unit to each hidden or output unit \cite{MLDict}.
Moreover, \textbf{activation function} is the function that describes the output of a neuron. Most network architecture starts by computing the weighted sum of the inputs. The total net input, is then usually transformed in some way, using activation function. The simplest activation function is a step function; If the total net input is less than 0 (or more generally, less than some threshold T) then the output of the neuron is 0, otherwise, it is 1. A common activation function is a logistic function \cite{MLDict}. The important characteristic of the activation function is that it provides a smooth, differentiable transition as input values change, i.e. small changes in input would produce small changes in output.\\
Mathematically, we can model a neuron as
\begin{equation}
\psi(XW+b).
\end{equation} 
Here, \textit{X} is the input vector, \textit{W} is the weight matrix, \textit{b} is the bias and finally, $\psi$ is the activation function. With this notation, the final output of our feedforward network would be
\begin{equation}
\Phi(X,W^1,\dots,W^K) = \psi_{k}(\psi_{k-1}(\dots \psi_{2}(\psi_{1}(XW^1+b^1)W^2+b^2)\dots W^{K-1}+b^{K-1})W^{K}+b^{K}).
\end{equation}
The initial input of the ANN is external data, such as images and documents. The ultimate output is to accomplish a task, such as recognizing an object within the image. To accomplish the task, the neural network would go through a process that we call learning. There are two general types of learning; learning from the data associated with labels which is called supervised learning and learning from unlabeled data which is called unsupervised learning. In this thesis, we would do supervised learning. \\
Learning, in case of supervised learning, is to minimize a function like $ \mathscr{L}(Y, \Phi(X, W^1,\dots, W^K))$ (i.e. \textbf{loss} function e.g. mean square error) that defines a relation between the network output $\Phi(X, W^1,\dots, W^K)$ and the expected network output \textit{Y}, which is the labels associated with the data
\begin{equation}
\min_{\{W^k\}_{k=1}^{K}} \mathscr{L}(Y, \Phi(X,W^1,\dots,W^K)).
\end{equation}
If the network were unable to minimize the loss function, we say that \textbf{underfitting} happened. To ensure that the network learned correctly, we have to test it against a fresh dataset which it hasn't see before, which called the test dataset. What we expect is an acceptable result from our network over this dataset concerning some metrics (e.g mean absolute error). If those results were satisfying, we say that the network learned successfully. If not, we would say that \textbf{overfitting} happened. It means that though the network has perfect results on the training dataset itself, it was unable to generalize the hidden relation of the training dataset (i.e. the provided data set in learning phase) to the test dataset.\\
%Often the weights of the network would be initialized randomly but as it's explained in Section \eqref{sec:overviewof_FEM_FDM}, we initialized our network with the zero matrices for weights. 
The learning process begins by feeding a \textbf{batch} of train data to the network. The network try to adjust it's weights so that it minimizes loss function. Then next batch will feed the network. Each passes through all of the train data is called an \textbf{epoch}. The whole learning process would often consist of many epochs. But one must be careful about overfitting the data with too many epochs.\\
There are two different choices to generate data sets when dealing with PDEs. First, solve the PDE over a bigger mesh to obtain the train data set and then solving the PDE over a finer mesh to generate test dataset. Second, picking a percentage of the data points concerning normal distribution over the domain. Moreover, to approach the problem, methods can be categorized into three categories. First, we know the PDE and its coefficients. What we seek is to solve the PDE directly using neural networks. In that regard, one may use the Ritz variational formulation of the PDE to define the loss function and uses the stochastic gradient decent (SGD) optimization algorithm as the optimizer. This way, a numerical approximation of the solution can be calculated much like any other numerical methods \cite{weinan2018deep}, or else, one may use a coarse grid to train the network and then use the trained model on a fine grid mesh \cite{wang2019prediction}. Second, the PDE is known but the coefficients are unknown \cite{raissi2017physics, Base_paper}. Finally, neither PDE nor it's coefficients are known \cite{raissi2018deep}. \\
Consider some PDEs with the random coefficient field, we are interested in finding our physical quantities as functionals which depend only on a few characteristic ``features'' of the coefficient fields through our neural network. A dimension reduction technique based on neural network representation has used to do regression.\\
Regression seeks to find a function $h_{\theta}$ parameterized by a parameter vector $\theta \in \mathbb{R}^{p}$ such that
\begin{equation}
f(\theta) \approx h_{\theta}(a), a\in \mathbb{R}^{q}
\label{regression_key_task}
\end{equation}
For example in linear regression, we need to handcraft a set of basis $\{\phi_k(a)\}$ such that $f(a) = \sum_{k} \beta_k \phi_k(a)$. However, this process exposes us to the risk of overfitting. A key advantage of the neural network is that it does not need a set of handcrafted basis functions; it bypasses that with learning a direct approximation of $f(a)$ that satisfies \eqref{regression_key_task} in a data driven way.\\
%However, choosing a sufficiently large class of approximation functions without the issue of over-fitting remains a delicate business. For example when choosing the set of basis $\{\phi_k(a)\}$ such that $f(a) = \sum_{k} \beta_k \phi_k(a)$ in linear regression.\\ 
%A key advantage of using neural network is that it bypasses the traditional need to handcraft basis for spanning $f(a)$ as in linear regression. Instead, it directly learns an approximation that satisfies \eqref{regression_key_task} in a data-driven way.
More precisely, we want to learn $f(a)$ that maps the random coefficient vector $a$ in a PDE to some physical quantities described by the PDE.\\
The approach is conceptually simple, consisting of the following steps
\begin{enumerate}[i.]
	\item Sample the random coefficients (a in \eqref{regression_key_task}) of the PDE from a user-specified distribution. For each set of coefficients, solve the deterministic PDE to obtain the physical quantity of interest ($f(a)$ in \eqref{regression_key_task}),
	\item Use a neural network as the surrogate model $h_{\theta}(a)$ in \eqref{regression_key_task} and train it using the previously obtained samples,
	\item  Validate the surrogate forward model with more samples. 
\end{enumerate}
We note that our work is a report of \cite{Base_paper}. In this thesis, the function that we want to parameterize is over the coefficient field of the PDE. 
%It would be beneficial to mention other works which try to solve deterministic PDE numerically using a neural network like \cite{weinan2018deep, khoo2019solving, lagaris1998artificial, long2017pde, rudd2015constrained, han2018solving}. \\
Now, back to the questions which this thesis is based upon. `Why one may interested in solving a PDE, especially in the presence of uncertainty, using neural network, and how?' To highlight the important aspect of the idea of using a neural network for solving PDEs, consider these questions. Would it be nice if we can solve our problem on a bigger mesh to train our network and then use it to compute the solution over a finer mesh? How about solving problems with high dimensions in which the classical numerical methods such as FEM and FDM suffer the curse of dimensionality? As we will see in the next section, computing the basis function over nodal points is the source of heavy computational cost in FEM. What if we would be able to bypass the need to compute them? These are just some of the applications which one may find when importing neural network as a tool in numerical analysis.\\
\section{An Overview of Classical Numerical Methods}
\label{sec:overviewof_FEM_FDM}
One of the popular  numerical method for solving PDEs is  Finite Difference Method (FDM). It is based on the idea of substituting the derivatives in the equation with their approximations. To do so, one may define a mesh, write down the value of each partial derivative on each nodal point of the mesh, and then calculating a numerical approximation for the solution of the partial differential equation. The precision of the method has a direct relation with the order of the precision which we chose to approximate the derivatives in and it can be computed through the Taylor expansion. Though the FDM is a good method and it is widely used, it has its downsides too. An important one is that the FDM cannot be used on complex domains \cite{forsythe1960finite, smith1985numerical, morton2005numerical}.\\
Finite Element Method (FEM) is based on the variational formulation of the equation and uses more complex mesh structures and it is utilized to investigate problems with more complex domains. One can either use \textbf{Ritz} formulation which writes an equivalent minimization problem or \textbf{Galerkin} formulation which uses the basics of integration by part and Green's identities to reduce the order of the differential equation. Either way, one would write an equivalent problem to the desired differential equation using basis functions. After achieving the variational formulation, one would approximate its solution by reducing the infinite dimension of the solution space to a finite dimension. Hence, the method is called \textbf{finite element method} \cite{suli2012lecture, johnson2012numerical}.\\
Although FEM is very popular, it cannot be used for high dimensional problems referred in literature as \textit{the curse of dimensionality}. Moreover, refinement of the mesh would result in increasing the nodal points count which means an increase in computational cost. Finally, FEM can explore more domains, yet the domain has to be Lipschitz. We would study finite element method in more details in Section \eqref{sec:finite_element_method} \cite{zienkiewicz1977finite, cook2007concepts}.\\
For the enthusiastic reader, here is a list of some other numerical methods: wavelet method \cite{dahmen1997multiscale, lepik2005numerical}, finite volume method \cite{moukalled2016finite, leveque2002finite} and meshfree method \cite{liu2003smoothed, liu2009meshfree}.
\section{Problem Statement and Contributions}
As we saw in Section \eqref{sec:PDE}, PDEs come to life to express the relations between changing quantities. In this section, we discuss a bit about the problems which we want to solve.\\
Suppose we have inhomogeneous media (inhomogeneous media are media in which all properties of interest are not the same at any point). Also, suppose that we have different values for conductance for different materials, which composed our media, modeled by $a(x)$. Given the fixed direction vector $\xi \in \mathbb{R}^d$, we are interested to know the effective conductance through the media in that direction. More precisely, we want to find the solution to the following minimization problem 
\begin{equation}
A_{\eff}(a) = \min_{u(x)} \int_{[0,1]^d} a(x) ||\nabla u(x) + \xi||_{2}^{2} \mathrm{d}x.
\label{problem:effective_conductance}
\end{equation}
For the second problem, we want to know the ground state energy of an electron along with its spatial distribution. Ground state energy is the state which electron wants to be in when the temperature drops to zero. It is equivalent to the smallest eigenvalue of the problem of the form
\begin{equation}
\label{general_Shrodinger_equation_form}
Hu = Eu,
\end{equation}
which in case of multi electrons, is
\begin{equation}
Eu(r) := [-\Delta + v(r) + \int w(r-r^*) |u(r^*)^2|dr^*]u(r). 
\end{equation}
In \eqref{general_Shrodinger_equation_form}, $H$ is a Hamiltonian operator which is the sum of the kinetic energies of all the particles, plus the potential energy of the particles associated with the system.
Considering multiple particles, our second equation
\begin{equation}\label{NLSE}
-\Delta u(x) + a(x)u(x) + \sigma u(x)^3 = E_0 u(x), ~ x\in [0,1]^d, ~s.t. \int_{[0,1]^d}u(x)^2 \mathrm{d}x = 1,
\end{equation}
can be derived from \eqref{general_Shrodinger_equation_form}. In \eqref{NLSE}, $-\Delta u(x)$ represents the kinetic energy, $a(x)$ is the potential.\\
The main contributions of this work are:
\begin{enumerate}[i.]
	\item Explaining the bottleneck of the existing numerical methods which led to high computational cost.
	\item Constructing a simple neural network with regular activation and loss functions to approximate the function of interest $f(a)$.
	\item Studying the effectiveness of the network on both linear (in 1D) and nonlinear (in 2D) cases.
	%\item Providing theoretical guarantees on the neural network representation of $f(a)$ in \eqref{regression_key_task} through explicit construction for the parametric PDE problems under study;
	%\item Showing that even a rather simple neural network architecture can learn a good representation of $f(a)$  in \eqref{regression_key_task} through training. 
\end{enumerate}
%\section{Results}
%\subsection{Effective Conductance in Inhomogeneus Media}
%In 1D, mean squared error measured to be $2.5793 \times 10^{-5}$ on the training set and $5.20 \times 10^{-6}$ on test set. Finally, the predicted effective conductance by the network was $0.76800650$ with the error $\norm{mean(\mathscr{A}_{\text{eff-predicted}}) - mean(\mathscr{A}_{\text{eff-exact}})} = 1.02100 \times 10^{-3}$.\\
%\subsection{Nonlinear Shr\"{o}dinger Equation}
%In 2D, the mean squared error measured to be $0.0173$ on the training set and $0.01425044$ on the test data set. Finally, the mean of predicted ground state energy by the network was $10.17474556$ which has $7.235 \times 10^{-5}$ $L^2$-distance from the mean of the normalized ground state energy provided.
\section{Thesis Structure}
The rest of this thesis is structured as follows. In Chapter 2, the mathematical preliminaries needed to understand the thesis are presented. In Chapter 3, we propose our discretization and neural model for the problems. In Chapter 4 we use the proposed model in Chapter 3 to compute the solution and report the results and conclude.












